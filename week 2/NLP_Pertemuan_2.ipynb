{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBefyd6UGdgx"
      },
      "source": [
        "<center><strong>Natural Language Processing</strong><br />\n",
        "<strong><font color=\"blue\">Semester Genap 2024-2025</font></strong><br />\n",
        "</center>\n",
        "\n",
        "<strong>Outline pertemuan minggu ke-2</strong><br />\n",
        "<li> Tokenisasi </li>\n",
        "<li> Casefolding </li>\n",
        "<li> Stemming dan Lemmatization</li>\n",
        "<li> Part of Speech (POS) tagging </li>\n",
        "<li> Stopword removal </li>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbO33SmcGdhA"
      },
      "source": [
        "## Tokenisasi\n",
        "\n",
        "<p>Tokenisasi adalah pemisahan kata, simbol, frase, dan entitas penting lainnya (yang disebut sebagai token) dari sebuah teks untuk kemudian di analisa lebih lanjut. Token dalam NLP sering dimaknai dengan &quot;sebuah kata&quot;, walau tokenisasi juga bisa dilakukan ke kalimat, paragraf, atau entitas penting lainnya (misal suatu pola string DNA di Bioinformatika).</p>\n",
        "\n",
        "<p><strong>Mengapa perlu tokenisasi?</strong></p>\n",
        "\n",
        "<ul>\n",
        "\t<li>Langkah penting dalam preprocessing, menghindari kompleksitas mengolah langsung pada string asal.</li>\n",
        "\t<li>Menghindari masalah (semantic) saat pemrosesan model-model natural language.</li>\n",
        "\t<li>Suatu tahapan sistematis dalam merubah unstructured (text) data ke bentuk terstruktur yang lebih mudah di olah.</li>\n",
        "</ul>\n",
        "\n",
        "<p><img alt=\"\" src=\"figures\\2_Pipeline_Tokenization.png\" style=\"height:300px; width:768px\" /><br />\n",
        "[<a href=\"https://www.softwareadvice.com/resources/what-is-text-analytics/\" target=\"_blank\"><strong>Image Source</strong></a>]</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8xbYn0cGdhC"
      },
      "source": [
        "<h2 id=\"Tokenisasi-dengan-modul-NLTK\">Tokenisasi dengan modul NLTK</h2>\n",
        "<p>NLTK dapat melakukan tokenisasi pada level kata dan pada level kalimat</p>\n",
        "\n",
        "<p><strong>Kelebihan</strong>:</p>\n",
        "\n",
        "<ol>\n",
        "\t<li>Well established dengan dukungan bahasa yang beragam</li>\n",
        "\t<li>Salah satu modul NLP dengan fungsi terlengkap, termasuk WordNet</li>\n",
        "\t<li>Free dan mendapat banyak dukungan akademis.</li>\n",
        "</ol>\n",
        "\n",
        "<p><strong>Kekurangan</strong>:</p>\n",
        "\n",
        "<ol>\n",
        "\t<li>Murni Python: relatif lebih lambat</li>\n",
        "</ol>\n",
        "\n",
        "<p><big><strong><a href=\"https://www.nltk.org/\" target=\"_blank\">https://www.nltk.org/</a></strong></big></p>\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVY4cAQly8EC",
        "outputId": "0b9ac53b-4c4f-405c-99a3-39ae3a7ef9d7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GkD7PT4GdhD",
        "outputId": "e5f38da7-49ba-41eb-e7b3-d4dd07a881e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# tokenisasi kata\n",
        "from nltk import word_tokenize\n",
        "\n",
        "S = \"Hello, Mr. Man. He smiled!! This, i.e. that, is it.\"\n",
        "word_token = word_tokenize(S)\n",
        "\n",
        "print(word_token)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'Mr.', 'Man', '.', 'He', 'smiled', '!', '!', 'This', ',', 'i.e', '.', 'that', ',', 'is', 'it', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcHwLZExGdhG",
        "outputId": "9b11bfdb-4c5a-4fc3-8984-62533ed77741",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Mengapa tidak menggunakan fungsi split dari python?? apa bedanya?\n",
        "\n",
        "word_split = S.split()\n",
        "print(word_split)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello,', 'Mr.', 'Man.', 'He', 'smiled!!', 'This,', 'i.e.', 'that,', 'is', 'it.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsJGDzURGdhH",
        "outputId": "f3c7b78e-f51b-41db-c7d8-336a2457038f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# tokenisasi kalimat\n",
        "from nltk import sent_tokenize\n",
        "\n",
        "sentence_token = sent_tokenize(S)\n",
        "print(sentence_token)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello, Mr. Man.', 'He smiled!!', 'This, i.e.', 'that, is it.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIasco_JGdhI"
      },
      "source": [
        "<h3 id=\"Latihan:\"><font color=\"blue\">Latihan 1:</font></h3>\n",
        "\n",
        "<ul>\n",
        "\t<li>Apakah tanda baca seperti &quot;?&quot; atau &quot;!&quot; akan memisahkan kalimat?</li>\n",
        "\t<li>Apakah tanda &quot;carriage return&quot;/enter/ganti baris memisahkan kalimat?</li>\n",
        "\t<li>Apakah &quot;;&quot; memisahkan kalimat?</li>\n",
        "\t<li>Apakah tanda dash &quot;-&quot; memisahkan kata? Dalam bahasa Indonesia/Inggris?</li>\n",
        "</ul>\n",
        "\n",
        "<strong>Tips</strong>: Perhatikan bentuk <em>struktur data</em> &quot;output&quot; dari tokenisasi NLTK.<br />\n",
        "<strong>Catatan</strong>: pindah baris di Python string bisa dilakukan dengan menggunakan symbol &quot;\\n&quot;<br />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Jawaban Latihan 1\n",
        "\n",
        "1. Iya, Tanda tanya \"?\" dan tanda seru \"!\" biasanya dianggap sebagai pemisah kalimat dalam banyak algoritma tokenisasi, termasuk di NLTK. Jadi, kalimat yang diikuti dengan tanda tanya atau tanda seru akan dianggap sebagai pemisah antar kalimat.\n",
        "\n",
        "2. Kurang tepat, Tanda ini biasanya tidak dianggap sebagai pemisah kalimat oleh tokenizer default di NLTK. Namun, jika teks memiliki \"\\n\", itu akan dianggap sebagai pemisah baris atau paragraf.\n",
        "\n",
        "3. Titik koma \";\" tidak selalu dianggap sebagai pemisah kalimat dalam tokenisasi standar. Biasanya, ini lebih dipandang sebagai pemisah klausa atau bagian dalam kalimat yang lebih panjang.\n",
        "\n",
        "4. Tidak, Tanda dash \"-\" pada umumnya digunakan dalam kedua bahasa (Indonesia dan Inggris) untuk menghubungkan kata atau bagian kalimat, seperti dalam kata majemuk atau gabungan kata."
      ],
      "metadata": {
        "id": "fDJVkaTyzohF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiakVvFRGdhK"
      },
      "source": [
        "# Kerjakan latihan 1 pada cell berikut ini\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKg5CsjqGdhL"
      },
      "source": [
        "## Tokenisasi dengan modul Spacy\n",
        "<strong>Kelebihan</strong>:</p>\n",
        "<ol>\n",
        "\t<li>Di claim lebih cepat (C-based)</li>\n",
        "\t<li>License termasuk untuk komersil</li>\n",
        "\t<li>Dukungan bahasa yang lebih banyak dari NLTK (termasuk bahasa Indonesia*)</li>\n",
        "</ol>\n",
        "\n",
        "<p><strong>Kekurangan</strong>:</p>\n",
        "<ol>\n",
        "\t<li>Fungsi yang lebih terbatas (dibandingkan NLTK).</li>\n",
        "\t<li>Karena berbasis compiler, sehingga instalasi cukup menantang.</li>\n",
        "</ol>\n",
        "\n",
        "<p><big><strong><a href=\"https://spacy.io/\" target=\"_blank\">https://spacy.io/</a></strong></big></p>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "283fGuQtGdhN",
        "outputId": "8908beb7-783f-4710-f96b-a171b0105379"
      },
      "source": [
        "# Contoh tokenisasi kata menggunakan Spacy\n",
        "import spacy\n",
        "\n",
        "# Loading language model\n",
        "spacy_en = spacy.load('en')\n",
        "\n",
        "S = \"Hello, Mr. Man. He smiled!! This, i.e. that, is it.\"\n",
        "tokens = spacy_en(S)\n",
        "print( [token.text for token in tokens] )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hello', ',', 'Mr.', 'Man', '.', 'He', 'smiled', '!', '!', 'This', ',', 'i.e.', 'that', ',', 'is', 'it', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDEfmuP6GdhO",
        "outputId": "b85c5fcf-31a3-4b21-bda5-4523a5d25d72"
      },
      "source": [
        "# Contoh tokenisasi kalimat dengan Spacy\n",
        "sentence_tokens = spacy_en(S).sents\n",
        "print([str(sent) for sent in sentence_tokens])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hello, Mr. Man.', 'He smiled!!', 'This, i.e. that, is it.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4o4De78GdhP"
      },
      "source": [
        "<h3 id=\"Latihan:\"><font color=\"blue\">Latihan 2:</font></h3>\n",
        "\n",
        "<ul>\n",
        "\t<li>Apakah hasil tokenisasi Spacy = NLTK? Mengapa?</li>\n",
        "\t<li>Lakukan latihan seperti yang dilakukan sebelumnya dengan modul NLTK, apakah hasilnya sama dengan Spacy?</li>\n",
        "</ul>\n",
        "\n",
        "<p><strong>Tips</strong>: Perhatikan bentuk <em>struktur data</em> &quot;output&quot; dari tokenisasi Spacy juga berbeda dengan NLTK.<br />\n",
        "<strong>Catatan</strong>: Contoh sederhana ini menekankan perbedaan ilmu linguistik dan computational linguistic.</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1TAgGGaGdhQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52754a7f-2ce1-45c5-e2c1-5d8fced979d3"
      },
      "source": [
        "# Kerjakan latihan 2 pada cell berikut ini\n",
        "# untuk NLTK\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "S = \"Hello, Mr. Man. He smiled!! This, i.e. that, is it.\"\n",
        "\n",
        "# Tokenisasi kata\n",
        "tokens = word_tokenize(S)\n",
        "\n",
        "# Menampilkan hasil tokenisasi\n",
        "print(tokens)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'Mr.', 'Man', '.', 'He', 'smiled', '!', '!', 'This', ',', 'i.e', '.', 'that', ',', 'is', 'it', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_tokens_nltk = sent_tokenize(S)\n",
        "\n",
        "# Menampilkan hasil tokenisasi kalimat\n",
        "print(sentence_tokens_nltk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wj7s8kGt38rB",
        "outputId": "ffd8c9c7-be53-4474-9f71-5678d25b3848"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello, Mr. Man.', 'He smiled!!', 'This, i.e.', 'that, is it.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Jawaban Latihan 2\n",
        "\n",
        "1. Hasil dari tokenize menggunakan NLTK hampir sama dengan Spacy, tetapi ada sedikit perbedaan di kata ('i.e.') pada spacy kata ('i.e.) dianggap menjadi kesatuan sedangkan di NLTK itu dipisah menjadi ('i.e') dan ('.'). Karena Spacy memiliki model yang lebih canggih dan memanfaatkan konteks, jadi ia tahu bahwa \"i.e.\" adalah singkatan yang harus diperlakukan sebagai satu token."
      ],
      "metadata": {
        "id": "64QHMGQv2x1A"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUv9p0QJGdhR"
      },
      "source": [
        "## Tokenisasi dengan TextBlob\n",
        "<strong>Kelebihan</strong>:</p>\n",
        "<ol>\n",
        "\t<li>Sederhana &amp; mudah untuk digunakan/pelajari.</li>\n",
        "\t<li>Textblob objects punya behaviour/properties yang sama dengan string di Python.</li>\n",
        "\t<li>TextBlob dibangun dari kombinasi modul NLTK dan (Clips) Pattern</li>\n",
        "</ol>\n",
        "\n",
        "<p><strong>Kekurangan</strong>:</p>\n",
        "<ol>\n",
        "\t<li>Tidak secepat Spacy dan NLTK</li>\n",
        "\t<li>Language Model terbatas: English, German, French</li>\n",
        "</ol>\n",
        "\n",
        "<p>*Blob : Binary large Object</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_Ko97wKGdhS",
        "outputId": "6b7d7f6d-bdfb-4962-fb00-35ebb07f6e96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Contoh tokenisasi dengan TextBlob\n",
        "from textblob import TextBlob\n",
        "\n",
        "T = \"Hello, Mr. Man. He smiled!! This, i.e. that, is it.\"\n",
        "sentence_tokens = TextBlob(T).sentences\n",
        "\n",
        "# Tokenisasi kata\n",
        "print(TextBlob(T).words)\n",
        "\n",
        "# Tokenisasi kalimat\n",
        "print([str(sent) for sent in sentence_tokens])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'Mr', 'Man', 'He', 'smiled', 'This', 'i.e', 'that', 'is', 'it']\n",
            "['Hello, Mr. Man.', 'He smiled!!', 'This, i.e.', 'that, is it.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgyUgtWZGdhT"
      },
      "source": [
        "<h3 id=\"Latihan:\"><font color=\"blue\">Latihan 3:</font></h3>\n",
        "\n",
        "<ul>\n",
        "\t<li>Ada yang berbeda dari hasilnya?&nbsp;Apakah lebih baik seperti ini?</li>\n",
        "</ul>\n",
        "\n",
        "<p><strong>Tips</strong>: TextBlob biasa digunakan untuk prototyping pada data yang tidak terlalu besar.<br />\n",
        "<strong>Catatan</strong>: Hati-hati tipe data Blob tidak biasa (objek).</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Jawaban Latihan 3\n",
        "\n",
        "1. Hasilnya menunjukkan bahwa hasil dari NLTK dan Text blob sama, berbeda dengan Spacy, menurut saya Jika tujuan kita adalah untuk menangani teks yang lebih alami dan akurat dalam konteks kalimat, Spacy adalah pilihan yang lebih baik karena ia lebih pintar dalam menangani konteks semantik, seperti singkatan \"i.e.\". Namun, NLTK dan TextBlob mungkin lebih berguna untuk tugas yang lebih sederhana"
      ],
      "metadata": {
        "id": "9WA8Xoqi50Pc"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6O1a7rfGdhU"
      },
      "source": [
        "# Kerjakan latihan 3 pada cell berikut ini\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdwNjoHbGdhV"
      },
      "source": [
        "## Tokenisasi: language dependent dan environment dependent\n",
        "\n",
        "<p>Tokenization sebenarnya tidak sesederhana memisahkan berdasarkan spasi dan removing symbol. Sebagai contoh dalam bahasa Jepang/Cina/Arab suatu kata bisa terdiri dari beberapa karakter.</p>\n",
        "\n",
        "<p><img alt=\"\" src=\"figures/2_Tokenization_Complexity.jpg\" style=\"height:500px; width:686px\" /><br />\n",
        "[<a href=\"http://aclweb.org/anthology/Y/Y11/Y11-1038.pdf\" target=\"_blank\"><strong>Image Source</strong></a>]</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDkd2CQkGdhW"
      },
      "source": [
        "__Bagaimana untuk data yang punya karakteristik tertentu seperti Twitter?__\n",
        "__Apakah bisa menggunakan modul tokenisasi yang telah tersedia?__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHpcLCQnGdhX",
        "outputId": "96e445a1-9a43-4f34-bc5f-a06d707a223b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Contoh Tokenizer untuk twitter\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "Tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
        "tweet = \"@text_mining I am so happpppppppy, supeeeer happpy :) #imsohappy #happy\"\n",
        "print(Tokenizer.tokenize(tweet))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'so', 'happpy', ',', 'supeeer', 'happpy', ':)', '#imsohappy', '#happy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PR8qjXXVGdhY",
        "outputId": "ddcc962d-5457-4b38-eb48-a5a978a7782d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import itertools\n",
        "\n",
        "tweet = \"@text_mining I am so supeeeeeeer\"\n",
        "\n",
        "# Menghilangkan double karakter\n",
        "tweet_clear = ''.join(''.join(s)[:1] for _, s in itertools.groupby(tweet))\n",
        "print(tweet_clear)\n",
        "\n",
        "# NOTES: untuk beberapa data spesifik seperti data bioinformatics, cryptography,\n",
        "# twitter, dst dibutuhkan tokenizer custom untuk dapat memenuhi kebutuhan"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@text_mining I am so super\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrF6c6VgGdhZ"
      },
      "source": [
        "## NLP Bahasa Indonesia:\n",
        "\n",
        "<p>module NLP Python yang support bahasa Indonesia secara umum masih sangat langka. Beberapa <u><strong>resources </strong></u>yang dapat digunakan:</p>\n",
        "\n",
        "<ol>\n",
        "\t<li><strong><a href=\"https://github.com/kirralabs/indonesian-NLP-resources\" target=\"_blank\">KirraLabs</a></strong>: Mix of NLP-TextMining resources</li>\n",
        "\t<li><strong><a href=\"https://pypi.python.org/pypi/Sastrawi/1.0.1\" target=\"_blank\">Sastrawi 1.0.1</a>:</strong>&nbsp;untuk &quot;stemming&quot; &amp;&nbsp;<strong><a href=\"https://devtrik.com/python/stopword-removal-bahasa-indonesia-python-sastrawi/\" target=\"_blank\">stopwords&nbsp;</a></strong>bahasa Indonesia.</li>\n",
        "\t<li><strong><a href=\"http://stop-words-list-bahasa-indonesia.blogspot.co.id/2012/09/daftar-kata-dasar-bahasa-indonesia.html\" target=\"_blank\">Daftar Kata Dasar Indonesia</a></strong>:&nbsp;Bisa di load sebagai dictionary di Python</li>\n",
        "\t<li><strong><a href=\"https://id.wiktionary.org/wiki/Wiktionary:ProyekWiki_bahasa_Indonesia/Daftar_kata\" target=\"_blank\">Wiktionary</a></strong>: ProyekWiki bahasa Indonesia [termasuk Lexicon]</li>\n",
        "\t<li><a href=\"http://wn-msa.sourceforge.net/\" target=\"_blank\"><strong>WordNet Bahasa Indonesia</strong></a>: Bisa di load&nbsp;sebagai dictionary (atau NLTK<em>*</em>) di Python.</li>\n",
        "\t<li><strong><a href=\"http://kakakpintar.com/daftar-kata-baku-dan-tidak-baku-a-z-dalam-bahasa-indonesia/\" target=\"_blank\">Daftar Kata Baku-Tidak Baku</a></strong>: Bisa di load sebagai dictionary di Python.</li>\n",
        "\t<li><strong><a href=\"https://spacy.io/\" target=\"_blank\">Spacy</a></strong>: Cepat/efisien, MIT License, tapi language model Indonesia masih terbatas.</li>\n",
        "\t<li><a href=\"http://ufal.mff.cuni.cz/udpipe\" target=\"_blank\"><strong>UdPipe</strong></a>: Online request &amp; restricted license (support berbagai bahasa -&nbsp;pemrograman).</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3HzeEHbGdhc",
        "outputId": "305c44b9-e22f-4e63-deaf-dcacabab5413",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Contoh Tokenisasi dalam bahasa Indonesia dengan Spacy\n",
        "from spacy.lang.id import Indonesian\n",
        "from spacy.lang.en import English\n",
        "\n",
        "# load language model bahasa Indonesia\n",
        "spacy_id = Indonesian()\n",
        "spacy_en = English()\n",
        "\n",
        "S = 'Sore itu, Hamzah melihat kupu-kupu di taman. Ibu membeli oleh-oleh di pasar'\n",
        "word_token = spacy_id(S)\n",
        "print([token.text for token in word_token])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sore', 'itu', ',', 'Hamzah', 'melihat', 'kupu-kupu', 'di', 'taman', '.', 'Ibu', 'membeli', 'oleh-oleh', 'di', 'pasar']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYJUtQDmGdhd",
        "outputId": "26569369-b7c8-4d88-e2e2-87c39bba8776",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Jika menggunakan Language model English:\n",
        "word_token_en = spacy_en(S)\n",
        "print([token.text for token in word_token_en])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sore', 'itu', ',', 'Hamzah', 'melihat', 'kupu', '-', 'kupu', 'di', 'taman', '.', 'Ibu', 'membeli', 'oleh', '-', 'oleh', 'di', 'pasar']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTdYrRpxGdhe"
      },
      "source": [
        "<h3 id=\"Latihan:\"><font color=\"blue\">Latihan 4:</font></h3>\n",
        "\n",
        "<ul>\n",
        "\t<li>Apakah ada perbedaan apabila menggunakan language model yang berbeda?</li>\n",
        "    <li>Bagaimana jika melakukan tokenisasi Bahasa Indonesia dengan NLTK? Apakah hasilnya sama dengan Spacy?\n",
        "</ul>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYhrGfOqGdhf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8a02c82-680c-4e3f-dd16-ce0e472ee6f4"
      },
      "source": [
        "# Kerjakan Latihan 4 pada cell berikut ini\n",
        "# untuk NLTk\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "S = 'Sore itu, Hamzah melihat kupu-kupu di taman. Ibu membeli oleh-oleh di pasar'\n",
        "\n",
        "# Tokenisasi menggunakan NLTK\n",
        "tokens_nltk = word_tokenize(S)\n",
        "print(tokens_nltk)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sore', 'itu', ',', 'Hamzah', 'melihat', 'kupu-kupu', 'di', 'taman', '.', 'Ibu', 'membeli', 'oleh-oleh', 'di', 'pasar']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Jawaban Latihan 4\n",
        "\n",
        "1. iya, perbedaan dalam penggunaan language model pada Spacy untuk bahasa Indonesia dan bahasa Inggris bisa menghasilkan hasil tokenisasi yang berbeda. misal spacy_id itu untuk indonesia sedangkan spacy_en itu untuk bahasa inggris\n",
        "\n",
        "2. Untuk perbedaan tidak ada kedunya mampu memisah kalimat majemuk seperti kupu-kupu dan oleh oleh dengan baik, dari NLTK maupun Spacy"
      ],
      "metadata": {
        "id": "iX9v2MXJ7Gra"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHcd3fiSGdhg"
      },
      "source": [
        "## Casefolding\n",
        "\n",
        "<p> Casefolding dilakukan untuk merubah karakter ke dalam huruf besar (uppercase) atau huruf kecil (lowercase) </p>\n",
        "\n",
        "<ul>\n",
        "\t<li>Untuk menganalisa makna (<em>semantic</em>) dari suatu (frase) kata dan mencari informasi dalam proses textmining, seringnya kita tidak membutuhkan informasi huruf besar/kecil dari kata&nbsp;tersebut.</li>\n",
        "    <li>Casefolding dapat dilakukan dengan efisien tanpa melalui proses tokenisasi</li>\n",
        "\t<li>Namun, bergantung pada analisa teks yang akan digunakan pengguna harus berhati-hati dengan urutan proses (pipelining) dalam preprocessing. </li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Eo1d58WGdhg",
        "outputId": "df1609c0-6a36-46aa-fb70-07303433995d"
      },
      "source": [
        "# Contoh casefolding\n",
        "S = \"Hi there!, I am a student. Nice to meet you :)\"\n",
        "print(S.lower())\n",
        "print(S.upper())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hi there!, i am a student. nice to meet you :)\n",
            "HI THERE!, I AM A STUDENT. NICE TO MEET YOU :)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rog6T5AeGdhh"
      },
      "source": [
        "<h3 id=\"Latihan:\"><font color=\"blue\">Latihan 5:</font></h3>\n",
        "\n",
        "<ul>\n",
        "\t<li>Temukan minimal 2 pengecualian dimana huruf besar dan kecil mempengaruhi makna dalam pemrosesan teks</li>\n",
        "    <li>Mengapa casefolding dapat dilakukan secara efisien tanpa melalui tahap tokenisasi?</li>\n",
        "    <li>Berikan contoh pengaruh dari urutan proses dalam preprocessing yang berpengaruh terhadap hasil preprocessing </li>\n",
        "        \n",
        "</ul>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Jawaban Latihan 5\n",
        "\n",
        "1. Nama atau singkatan khusus\n",
        "ex = us vs US -->  \"us\" dapat merujuk pada kata ganti orang pertama \"kami\" dalam bahasa Inggris, sedangkan \"US\" adalah singkatan dari \"United States\" (Amerika Serikat).\n",
        "\n",
        "Akronim atau singkatan\n",
        "contoh : \"HIV\" vs \"hiv\":\n",
        "\"HIV\" adalah singkatan dari Human Immunodeficiency Virus, virus yang menyebabkan AIDS.\n",
        "\"hiv\" dengan huruf kecil bisa dianggap sebagai kesalahan atau tidak dikenali sebagai akronim yang valid.\n",
        "\n",
        "2. Casefolding dapat dilakukan secara efisien tanpa melalui tahap tokenisasi karena casefolding berfungsi pada level karakter dalam teks, sedangkan tokenisasi berfokus pada pemisahan teks menjadi unit-unit yang lebih besar, seperti kata atau kalimat.\n",
        "\n",
        "3. Urutan dalam preprocessing teks sangat mempengaruhi hasil akhir. Beberapa langkah, seperti casefolding, stopwords removal, lemmatization, dan menghapus tanda baca, jika dilakukan secara berantakan misalnya stemming duluan kita bakal kehilangan berbagai informasi yang terdapat pada text"
      ],
      "metadata": {
        "id": "_SR2ETva77ui"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNzLEaqMGdhh"
      },
      "source": [
        "# Kerjakan latihan 5 pada cell berikut ini\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FwO1Ta-Gdhi"
      },
      "source": [
        "## Stemming dan Lemmatization\n",
        "\n",
        "<ol>\n",
        "\t<li>\n",
        "\t<p><strong>Stemmer</strong>&nbsp;akan menghasilkan sebuah bentuk kata yang disepakati oleh suatu sistem tanpa mengindahkan konteks kalimat. Syaratnya beberapa kata dengan makna serupa hanya perlu dipetakan secara konsisten ke sebuah kata baku.&nbsp;Banyak digunakan di IR &amp;&nbsp;komputasinya relatif sedikit. Biasanya dilakukan dengan menghilangkan imbuhan (suffix/prefix).</p>\n",
        "\t</li>\n",
        "\t<li>\n",
        "\t<p><strong>lemmatisation</strong> akan menghasilkan kata baku (dictionary word) dan bergantung konteks.</p>\n",
        "\t</li>\n",
        "\t<li>\n",
        "\t<p>Lemma &amp; stemming bisa jadi sama-sama menghasilkan suatu akar kata (root word). Misal : <em>Melompat </em>==&gt; <em>lompat</em></p>\n",
        "\t</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spXPmefnGdhi"
      },
      "source": [
        "<p><strong>Mengapa melakukan Stemming &amp; Lemmatisasi</strong>?</p>\n",
        "\n",
        "<ol>\n",
        "\t<li>Sering digunakan di IR (Information Retrieval) agar ketika seseorang mencari kata tertentu, maka seluruh kata yang terkait juga diikutsertakan.<br />\n",
        "\tMisal:&nbsp;<em>organize</em>,&nbsp;<em>organizes</em>, and&nbsp;<em>organizing&nbsp;</em>&nbsp;dan&nbsp;<em>democracy</em>,&nbsp;<em>democratic</em>, and&nbsp;<em>democratization</em>.</li>\n",
        "\t<li>Di Text Mining Stemming dan Lemmatisasi akan mengurangi dimensi (mengurangi variasi morphologi), yang terkadang akan meningkatkan akurasi.</li>\n",
        "\t<li>Tapi di IR efeknya malah berkebalikan: <strong><font color=\"blue\">meningkatkan recall, tapi menurunkan akurasi&nbsp;</font></strong>[<a href=\"https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\" target=\"_blank\"><strong>Link</strong></a>]. Contoh: kata&nbsp;<em>operate, operating, operates, operation, operative, operatives, dan operational</em>&nbsp;jika di stem menjadi <em>operate</em>, maka ketika seseorang mencari &quot;<em>operating system</em>&quot;, maka entry seperti&nbsp;<em>operational and research</em> dan&nbsp;<em>operative and dentistry</em>&nbsp;akan muncul sebagai entry dengan relevansi yang cukup tinggi.</li>\n",
        "</ol>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2fX_F8VGdhi"
      },
      "source": [
        "__Stemming tidak perlu \"benar\", hanya perlu konsisten__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW62Po_kGdhj"
      },
      "source": [
        "## NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rToCTObtGdhj",
        "outputId": "33c2c7c5-fdbf-4fd9-fd3a-dcf8c0d55a9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "source": [
        "# Contoh Stemming di NLTK\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.snow import SnowballStemmer\n",
        "\n",
        "S = 'presumably I would like to MultiPly my provision, saying tHat without crYing'\n",
        "print('Sentence: ',S)\n",
        "\n",
        "stemmer_list = [LancasterStemmer, PorterStemmer, SnowballStemmer]\n",
        "names = ['Lancaster', 'Porter', 'SnowBall']\n",
        "for stemmer_name,stem in zip(names,stemmer_list):\n",
        "    if stemmer_name == 'SnowBall':\n",
        "        st = stem('english')\n",
        "    else:\n",
        "        st = stem()\n",
        "    print(stemmer_name,': ',' '.join(st.stem(s) for s in S.split()))\n",
        "# perhatikan, kita tidak melakukan case normalization (lowercase)\n",
        "# Hasil stemming bisa tidak bermakna"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'nltk.stem.snow'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-b532be73842e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlancaster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLancasterStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'presumably I would like to MultiPly my provision, saying tHat without crYing'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk.stem.snow'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ww2jyHXqGdhk",
        "outputId": "68eb18f7-61fa-40cd-e3ea-11326e51640f"
      },
      "source": [
        "# Contoh Lemmatizer di NLTK\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "S = \"apples and oranges are similar. boots and hippos aren't, don't you think?\"\n",
        "print('Sentence: ', S)\n",
        "print('Lemmatize: ',' '.join(lemmatizer.lemmatize(s) for s in S.split()))\n",
        "# Lemma case sensitive. Dengan kata lain string harus diubah ke dalam bentuk huruf kecil (lower case)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence:  apples and oranges are similar. boots and hippos aren't, don't you think?\n",
            "Lemmatize:  apple and orange are similar. boot and hippo aren't, don't you think?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPZ1opHHGdhl",
        "outputId": "6f8a92d6-f159-49dd-888a-fe7671bb9015"
      },
      "source": [
        "# Lemmatizer menggunakan informasi pos. \"pos\" (part-of-speech) akan dibahas di segmen berikutnya\n",
        "print(lemmatizer.lemmatize(\"better\", pos=\"a\")) # adjective\n",
        "print(lemmatizer.lemmatize(\"better\", pos=\"v\")) # verb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "good\n",
            "better\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ISmjsdMGdhl"
      },
      "source": [
        "## TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaZa3o97Gdhm",
        "outputId": "a9c67489-dac1-4247-bf31-63023ab94c2f"
      },
      "source": [
        "# Contoh TextBlob Stemming & Lemmatizer\n",
        "from textblob import Word\n",
        "# Stemming\n",
        "print(\"Stem: \", Word('running').stem()) # menggunakan NLTK Porter stemmer\n",
        "\n",
        "# Lemmatizer\n",
        "print(\"Lemmatize: \", Word('went').lemmatize('v'))\n",
        "\n",
        "# default Noun, plural akan menjadi singular dari akar katanya\n",
        "# Juga case sensitive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stem:  run\n",
            "Lemmatize:  go\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-GaTcnsGdhm"
      },
      "source": [
        "## Spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sd2Oa6XWGdhn",
        "outputId": "0cfeda7f-7d3d-4788-f018-9a051a2511bd"
      },
      "source": [
        "# Spacy Lemmatizer English\n",
        "sent = \"I am sure Apples and oranges are similar. Boots and hippos aren't, don't you think?\"\n",
        "sent_token = spacy_en(sent)\n",
        "print( ' '.join( s.lemma_ for s in sent_token ) )\n",
        "# HATI-HATI dengan lemma \"I\" dan \"you\" di Spacy!!!..."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-PRON- be sure apples and orange be similar . boot and hippo be not , do not -PRON- think ?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVqT805NGdho",
        "outputId": "4e82e327-393e-4013-806d-1032cda196f7"
      },
      "source": [
        "# Spacy Lemmatizer Indonesia\n",
        "I = \"perayaan itu berbarengan dengan saat kita bepergian ke Jogjakarta\"\n",
        "idn = spacy_id(I)\n",
        "print( ' '.join( k.lemma_ for k in idn ) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "raya itu bareng dengan saat kita pergi ke Jogjakarta\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTX9OEwhGdho",
        "outputId": "1c6c15cc-0a3f-4dec-cf26-2d736769508e"
      },
      "source": [
        "# Perhatikan output berikut (hati-hati inkonsistensi)\n",
        "print([k.lemma_ for k in spacy_id(\"Perayaan Bepergian\")])\n",
        "\n",
        "# bagaimana dengan Spacy stemmer??\n",
        "# Spacy belum support stemmer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Perayaan', 'Bepergian']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwunTiGrGdhp"
      },
      "source": [
        "## Sastrawi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IuF7tEOGdhq",
        "outputId": "9433e372-9aee-4ff2-d76d-f3cb953eb82a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Lemmatizer dengan Sastrawi\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "stemmer = StemmerFactory().create_stemmer()\n",
        "\n",
        "I = \"perayaan itu berbarengan dengan saat kita bepergian ke Makassar\"\n",
        "print(stemmer.stem(I))\n",
        "print(stemmer.stem(\"Perayaan Bepergian Menyuarakan\"))\n",
        "# Ada beberapa hal yang berbeda antara Sastrawi dan modul-modul diatas.\n",
        "# Apa sajakah?"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "raya itu bareng dengan saat kita pergi ke makassar\n",
            "raya pergi suara\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAuO4vnPGdhr"
      },
      "source": [
        "<h3 id=\"Tips:\">Tips:</h3>\n",
        "\n",
        "<ul>\n",
        "\t<li>Secara umum &#39;biasanya&#39; di Text Mining yang kita butuhkan hanyalah <strong><font color=\"blue\">Lemma</font></strong>.</li>\n",
        "\t<li>&quot;Kecuali&quot; di aplikasi IR, spelling correction, variasi kata, clustering, atau terkadang klasifikasi. Pada aplikasi-aplikasi tersebut stemming terkadang lebih diinginkan.</li>\n",
        "\t<li>Stemming jauh lebih cepat, tapi tidak selalu tersedia di modul NLP.</li>\n",
        "\t<li>Beberapa algoritma tertentu membutuhkan tanda &quot;.&quot; dan &quot;,&quot; : contohnya untuk document summarization di textRank.</li>\n",
        "\t<li>&quot;_&quot; juga biasa digunakan untuk menyatakan frase kata di representasi n-grams (misal &quot;buah_tangan&quot;).</li>\n",
        "\t<li>Stemming juga digunakan pada Word Sense Disambiguation (WSD)</li>\n",
        "</ul>\n",
        "\n",
        "<h3 id=\"Diskusi:\">Diskusi:</h3>\n",
        "\n",
        "<ul>\n",
        "\t<li>Untuk menghemat storage database, apakah sebaiknya kita menyimpan saja hasil preprocessed texts/documents?</li>\n",
        "</ul>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVwGUT8iGdhr"
      },
      "source": [
        "## Part-of-Speech (POS) tag\n",
        "\n",
        "<p> POS tagging merupakan proses pemberian tanda berupa kelas kata pada setiap kata yang terdapat di dalam corpus.</p>\n",
        "\n",
        "<p><img alt=\"\" src=\"figures/2_parts-of-speech-chart.jpg\" style=\"height:400px; width:404px\" /></p>\n",
        "<p>[<a href=\"https://www.paperrater.com/page/parts-of-speech\" target=\"_blank\">image source</a>]</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlAoEM0fGdhs",
        "outputId": "787b2a9c-ef06-4829-d83c-b0018f174aa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        }
      },
      "source": [
        "# Contoh POS tags dengan NLTK (bahasa Inggris)\n",
        "from nltk import pos_tag\n",
        "S = 'I am currently learning NLP in English, but if possible I want to know NLP in Indonesian language too'\n",
        "\n",
        "tokens = word_tokenize(S)\n",
        "print(pos_tag(tokens))\n",
        "# Tidak lagi hanya 9 macam tags seperti yang dibahas ahli bahasa (linguist)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-c045d1200a75>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# Tidak lagi hanya 9 macam tags seperti yang dibahas ahli bahasa (linguist)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \"\"\"\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m_get_tagger\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, load, lang)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_conf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_tagdict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mload_from_json\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"eng\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;31m# Automatically find path to the tagger if location is not specified.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"taggers/averaged_perceptron_tagger_{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mTAGGER_JSONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"weights\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmnZ1JmFGdht"
      },
      "source": [
        "### Daftar POS tag NLTK:\n",
        "\n",
        "<img alt=\"\" src=\"figures/2_post_tags_NLTK.png\" style=\"height:400px; width:516px\" /></h3>\n",
        "\n",
        "<p>[<a href=\"http://gitqwerty777.github.io/natural-language-processing/\" target=\"_blank\">image source</a>]</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dasYYZLLGdht",
        "outputId": "f2fa4bc6-c1e4-41eb-e11b-6e313584f0fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 928
        }
      },
      "source": [
        "# Contoh POS tag dengan TextBlob pada bahasa Inggris\n",
        "for word, pos in TextBlob(T).tags:\n",
        "    print(word, pos, end=', ')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MissingCorpusError",
          "evalue": "\nLooks like you are missing some required data for this feature.\n\nTo download the necessary data, simply run\n\n    python -m textblob.download_corpora\n\nor use the NLTK downloader to download the missing data: http://nltk.org/data.html\nIf this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/textblob/decorators.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/textblob/en/taggers.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \"\"\"\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m_get_tagger\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, load, lang)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mload_from_json\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;31m# Automatically find path to the tagger if location is not specified.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"taggers/averaged_perceptron_tagger_{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mTAGGER_JSONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"weights\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mMissingCorpusError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-17f109e5ec5f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Contoh POS tag dengan TextBlob pada bahasa Inggris\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m', '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/textblob/decorators.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/textblob/blob.py\u001b[0m in \u001b[0;36mpos_tags\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    501\u001b[0m             return [\n\u001b[1;32m    502\u001b[0m                 \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tags\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             ]\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/textblob/blob.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    501\u001b[0m             return [\n\u001b[1;32m    502\u001b[0m                 \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tags\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             ]\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/textblob/decorators.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/textblob/blob.py\u001b[0m in \u001b[0;36mpos_tags\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    507\u001b[0m             return [\n\u001b[1;32m    508\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mWord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mPUNCTUATION_REGEX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m             ]\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/textblob/decorators.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMissingCorpusError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMissingCorpusError\u001b[0m: \nLooks like you are missing some required data for this feature.\n\nTo download the necessary data, simply run\n\n    python -m textblob.download_corpora\n\nor use the NLTK downloader to download the missing data: http://nltk.org/data.html\nIf this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tz5jRrXyGdhu",
        "outputId": "df0f43c3-fe9a-4bfb-a246-8c2ffc3df23e"
      },
      "source": [
        "# Contoh POS tag dengan Spacy pada bahasa Inggris\n",
        "tokens = spacy_en(T)\n",
        "for token in tokens:\n",
        "    print(token,token.tag_, end =', ')\n",
        "\n",
        "# Spacy belum support POS tag untuk bahasa Indonesia"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello UH, , ,, Mr. NNP, Man NNP, . ., He PRP, smiled VBD, ! ., ! ., This DT, , ,, i.e. FW, that DT, , ,, is VBZ, it PRP, . ., "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yOD2-8tGdhv",
        "outputId": "35c45dcf-7251-4589-b93b-5daceef61471"
      },
      "source": [
        "# Untuk mengetahui arti dari POS tag pada Spacy dapat menggunakan perintah \"explain\"\n",
        "spacy.explain('RB')\n",
        "# Daftar Lengkap: https://spacy.io/api/annotation#pos-tagging"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'adverb'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gi-9cierGdhw",
        "outputId": "40f09d46-ee92-4bd6-e610-09acbe809d45"
      },
      "source": [
        "# POS tag bahasa Indonesia dengan NLTK\n",
        "# https://yudiwbs.wordpress.com/2018/02/20/pos-tagger-bahasa-indonesia-dengan-pytho/\n",
        "from nltk.tag import CRFTagger\n",
        "ct = CRFTagger()\n",
        "ct.set_model_file('data/all_indo_man_tag_corpus_model.crf.tagger')\n",
        "\n",
        "hasil = ct.tag_sents([['Saya','bekerja','di','Bandung']])\n",
        "print(hasil)\n",
        "# Hati-hati dengan struktur data inputnya"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[('Saya', 'PRP'), ('bekerja', 'VB'), ('di', 'IN'), ('Bandung', 'NNP')]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTjcdke8Gdhw"
      },
      "source": [
        "<h3 id=\"Latihan:\"><font color=\"blue\">Latihan 6:</font></h3>\n",
        "\n",
        "<ul>\n",
        "\t<li>Kapan harus melakukan POS tagging pada tahap preprocessing?</li>\n",
        "    <li>Buatlah contoh hasi dari POS tag dengan hanya mengambil kata yang memiliki tag NOUN (*), dan berikan contoh kasus penggunaannya?</li>\n",
        "    <li>Buatlah contoh hasil dari POS tag yang telah ditambahkan pada setiap kata dalam suatu kalimat dengan menggunakan NLTK (**)</li>\n",
        "        \n",
        "</ul>\n",
        "\n",
        "<p>(*) <strong>Input</strong>: \"The tiger (Panthera tigris) is the largest extant cat species and a member of the genus Panthera. It is most recognisable for its dark vertical stripes on orange-brown fur with a lighter underside. It is an apex predator, primarily preying on ungulates such as deer and wild boar.\"</p>\n",
        "<p>(**)</p>\n",
        "<p> <strong>Input</strong>: \"I am currently learning NLP in English, but if possible I want to know NLP in Indonesian language too\"</p>\n",
        "<p> <strong>Expected output</strong>: \"I_PRP am_VBP currently_RB learning_VBG NLP_NNP in_IN English_NNP ,_, but_CC if_IN possible_JJ I_PRP want_VBP to_TO know_VB NLP_NNP in_IN Indonesian_JJ language_NN too_RB\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Jawaban Latihan 6\n",
        "\n",
        "1. POS (Part-of-Speech) tagging biasanya dilakukan setelah beberapa langkah preprocessing dasar seperti tokenization, case folding, dan penghapusan stopwords. POS tagging penting untuk menganalisis struktur gramatikal teks dan mengidentifikasi kata berdasarkan kategorinya (misalnya, kata benda, kata kerja, kata sifat, dll.)."
      ],
      "metadata": {
        "id": "xjQFUz2G-YVG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tk6gqjD1Gdhx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9260443-d7fc-4060-ea7d-0c3d03c08c8f"
      },
      "source": [
        "# Kerjakan latihan 6 pada cell berikut ini\n",
        "# No 2\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "# Kalimat input\n",
        "sentence = \"The tiger (Panthera tigris) is the largest extant cat species and a member of the genus Panthera. It is most recognisable for its dark vertical stripes on orange-brown fur with a lighter underside. It is an apex predator, primarily preying on ungulates such as deer and wild boar.\"\n",
        "\n",
        "# Tokenisasi dan POS tagging\n",
        "tokens = word_tokenize(sentence)\n",
        "tagged = pos_tag(tokens)\n",
        "\n",
        "# Filter kata yang memiliki tag NOUN\n",
        "nouns = [word for word, tag in tagged if tag in ['NN', 'NNS', 'NNP', 'NNPS']]\n",
        "\n",
        "# Menampilkan hasil\n",
        "print(nouns)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['tiger', 'Panthera', 'tigris', 'cat', 'species', 'member', 'genus', 'Panthera', 'stripes', 'fur', 'underside', 'predator', 'ungulates', 'deer', 'boar']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Kalimat input\n",
        "sentence = \"I am currently learning NLP in English, but if possible I want to know NLP in Indonesian language too\"\n",
        "\n",
        "# Tokenisasi dan POS tagging\n",
        "tokens = word_tokenize(sentence)\n",
        "tagged = pos_tag(tokens)\n",
        "\n",
        "# Menampilkan hasil POS tagging\n",
        "print(tagged)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ombY7FBH_Qe4",
        "outputId": "fd161493-1ad2-49ef-e7cf-097bb2a1dce1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('I', 'PRP'), ('am', 'VBP'), ('currently', 'RB'), ('learning', 'VBG'), ('NLP', 'NNP'), ('in', 'IN'), ('English', 'NNP'), (',', ','), ('but', 'CC'), ('if', 'IN'), ('possible', 'JJ'), ('I', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('know', 'VB'), ('NLP', 'NNP'), ('in', 'IN'), ('Indonesian', 'JJ'), ('language', 'NN'), ('too', 'RB')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0bcli9SGdhy"
      },
      "source": [
        "## Stopword removal\n",
        "\n",
        "<p> Stopword removal merupakan salah satu cara untuk melakukan normalisasi pada level kata </p>\n",
        "<p><u>Di Text Mining</u> kata-kata yang <strong>sering muncul </strong>(dan jarang sekali muncul) memiliki sedikit sekali informasi (signifikansi) terhadap model (machine learning) yang digunakan. Hal ini di karenakan kata-kata tersebut muncul di semua kategori (di permasalahan klasifikasi) atau di semua cluster (di permasalahan pengelompokan/clustering). Kata-kata yang sering muncul ini biasa disebut &quot;StopWords&quot;. Stopwords berbeda-beda bergantung dari Bahasa dan Environment (aplikasi)-nya.<br />\n",
        "<strong>Contoh</strong>:<br />\n",
        "\n",
        "<ul>\n",
        "\t<li>Stopwords bahasa Inggris: am, is, are, do, the, of, etc.</li>\n",
        "\t<li>Stopwords bahasa Indonesia: adalah, dengan, yang, di, ke, dsb</li>\n",
        "\t<li>Stopwords twitter: RT, ...</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LhNtKI5Gdhy",
        "outputId": "0a8c8dd5-1ae0-404b-9bca-b77e9b9b9168",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Contoh stopword dari NLTK\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk_stw_en = stopwords.words('english')\n",
        "print(nltk_stw_en[:10])\n",
        "\n",
        "nltk_stw_id = stopwords.words('indonesian')\n",
        "print(nltk_stw_id[:10])\n",
        "\n",
        "# Lsit stopword dapat ditambahkan dan dikurangi sesuai dengan kebutuhan"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an']\n",
            "['ada', 'adalah', 'adanya', 'adapun', 'agak', 'agaknya', 'agar', 'akan', 'akankah', 'akhir']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "j3ksoYot_a_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Sastrawi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hL5Ly-zG_bbE",
        "outputId": "9bc3ece9-f6aa-4624-9fb3-8a3c584f112d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl.metadata (909 bytes)\n",
            "Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Sastrawi\n",
            "Successfully installed Sastrawi-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAvt50LIGdhz",
        "outputId": "355b73d6-d450-438c-8cd2-63af19992d3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#contoh stopword dari Sastrawi pada bahasa Indonesia\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "\n",
        "factory = StopWordRemoverFactory()\n",
        "sastrawi_stw_id = factory.get_stop_words()\n",
        "print(sastrawi_stw_id[:10])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['yang', 'untuk', 'pada', 'ke', 'para', 'namun', 'menurut', 'antara', 'dia', 'dua']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65AwqowSGdhz"
      },
      "source": [
        "# Tips:\n",
        "# selalu rubah list stopwords ke bentuk set, karena di Python jauh lebih cepat untuk cek existence di set ketimbang list\n",
        "nltk_stw_en = set(nltk_stw_en)\n",
        "nltk_stw_id = set(nltk_stw_id)\n",
        "sastrawi_stw_id = set(sastrawi_stw_id)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uD2PwuQBGdh0"
      },
      "source": [
        "<h3 id=\"Latihan:\"><font color=\"blue\">Latihan 7:</font></h3>\n",
        "\n",
        "<p> Lakukan stopword removal pada contoh paragraf berikut ini: </p>\n",
        "<p> \"Siti Nurbaya adalah sebuah novel Indonesia yang ditulis oleh Marah Rusli. Novel ini diterbitkan oleh Balai Pustaka, penerbit nasional negeri Hindia Belanda, pada tahun 1922.\" </p>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVa7Im8nGdh0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b49564fc-d5d3-435a-a20f-61831bf16837"
      },
      "source": [
        "# Kerjakan latihan 7 pada cell berikut ini\n",
        "# Daftar stopwords bahasa Indonesia (dapat diperluas sesuai kebutuhan)\n",
        "stopwords_indonesia = set([\n",
        "    'yang', 'di', 'dan', 'dari', 'ke', 'untuk', 'adalah', 'pada', 'ini', 'sebuah', 'dengan', 'oleh', 'dalam', 'memiliki', 'atau'\n",
        "])\n",
        "\n",
        "# Paragraf input\n",
        "text = \"Siti Nurbaya adalah sebuah novel Indonesia yang ditulis oleh Marah Rusli. Novel ini diterbitkan oleh Balai Pustaka, penerbit nasional negeri Hindia Belanda, pada tahun 1922.\"\n",
        "\n",
        "# Tokenisasi\n",
        "tokens = text.split()\n",
        "\n",
        "# Menghapus stopwords\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stopwords_indonesia]\n",
        "\n",
        "# Menampilkan hasil\n",
        "print(filtered_tokens)\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Siti', 'Nurbaya', 'novel', 'Indonesia', 'ditulis', 'Marah', 'Rusli.', 'Novel', 'diterbitkan', 'Balai', 'Pustaka,', 'penerbit', 'nasional', 'negeri', 'Hindia', 'Belanda,', 'tahun', '1922.']\n"
          ]
        }
      ]
    }
  ]
}