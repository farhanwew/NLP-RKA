{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBefyd6UGdgx"
      },
      "source": [
        "<center><strong>Natural Language Processing</strong><br />\n",
        "<strong><font color=\"blue\">Semester Genap 2024-2025</font></strong><br />\n",
        "</center>\n",
        "\n",
        "<strong>Outline pertemuan minggu ke-2</strong><br />\n",
        "<li> Tokenisasi </li>\n",
        "<li> Casefolding </li>\n",
        "<li> Stemming dan Lemmatization</li>\n",
        "<li> Part of Speech (POS) tagging </li>\n",
        "<li> Stopword removal </li>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbO33SmcGdhA"
      },
      "source": [
        "## Tokenisasi\n",
        "\n",
        "<p>Tokenisasi adalah pemisahan kata, simbol, frase, dan entitas penting lainnya (yang disebut sebagai token) dari sebuah teks untuk kemudian di analisa lebih lanjut. Token dalam NLP sering dimaknai dengan &quot;sebuah kata&quot;, walau tokenisasi juga bisa dilakukan ke kalimat, paragraf, atau entitas penting lainnya (misal suatu pola string DNA di Bioinformatika).</p>\n",
        "\n",
        "<p><strong>Mengapa perlu tokenisasi?</strong></p>\n",
        "\n",
        "<ul>\n",
        "\t<li>Langkah penting dalam preprocessing, menghindari kompleksitas mengolah langsung pada string asal.</li>\n",
        "\t<li>Menghindari masalah (semantic) saat pemrosesan model-model natural language.</li>\n",
        "\t<li>Suatu tahapan sistematis dalam merubah unstructured (text) data ke bentuk terstruktur yang lebih mudah di olah.</li>\n",
        "</ul>\n",
        "\n",
        "<p><img alt=\"\" src=\"https://software-advice.imgix.net/blog/wordpress/Text-mining-pipeline.png?fm=auto\" style=\"height:300px; width:768px\" /><br />\n",
        "[<a href=\"https://www.softwareadvice.com/resources/what-is-text-analytics/\" target=\"_blank\"><strong>Image Source</strong></a>]</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8xbYn0cGdhC"
      },
      "source": [
        "<h2 id=\"Tokenisasi-dengan-modul-NLTK\">Tokenisasi dengan modul NLTK</h2>\n",
        "<p>NLTK dapat melakukan tokenisasi pada level kata dan pada level kalimat</p>\n",
        "\n",
        "<p><strong>Kelebihan</strong>:</p>\n",
        "\n",
        "<ol>\n",
        "\t<li>Well established dengan dukungan bahasa yang beragam</li>\n",
        "\t<li>Salah satu modul NLP dengan fungsi terlengkap, termasuk WordNet</li>\n",
        "\t<li>Free dan mendapat banyak dukungan akademis.</li>\n",
        "</ol>\n",
        "\n",
        "<p><strong>Kekurangan</strong>:</p>\n",
        "\n",
        "<ol>\n",
        "\t<li>Murni Python: relatif lebih lambat</li>\n",
        "</ol>\n",
        "\n",
        "<p><big><strong><a href=\"https://www.nltk.org/\" target=\"_blank\">https://www.nltk.org/</a></strong></big></p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVY4cAQly8EC",
        "outputId": "0b9ac53b-4c4f-405c-99a3-39ae3a7ef9d7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\farha\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\farha\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\farha\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\farha\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\farha\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GkD7PT4GdhD",
        "outputId": "e5f38da7-49ba-41eb-e7b3-d4dd07a881e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', ',', 'Mr.', 'Man', '.', 'He', 'smiled', '!', '!', 'This', ',', 'i.e', '.', 'that', ',', 'is', 'it', '.']\n"
          ]
        }
      ],
      "source": [
        "# tokenisasi kata\n",
        "from nltk import word_tokenize\n",
        "\n",
        "S = \"Hello, Mr. Man. He smiled!! This, i.e. that, is it.\"\n",
        "word_token = word_tokenize(S)\n",
        "\n",
        "print(word_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcHwLZExGdhG",
        "outputId": "9b11bfdb-4c5a-4fc3-8984-62533ed77741"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello,', 'Mr.', 'Man.', 'He', 'smiled!!', 'This,', 'i.e.', 'that,', 'is', 'it.']\n"
          ]
        }
      ],
      "source": [
        "# Mengapa tidak menggunakan fungsi split dari python?? apa bedanya?\n",
        "word_split = S.split()\n",
        "print(word_split)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "karena tidak bisa membedakan pungtuasi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsJGDzURGdhH",
        "outputId": "f3c7b78e-f51b-41db-c7d8-336a2457038f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello, Mr. Man.', 'He smiled!!', 'This, i.e.', 'that, is it.']\n"
          ]
        }
      ],
      "source": [
        "# tokenisasi kalimat\n",
        "from nltk import sent_tokenize\n",
        "\n",
        "sentence_token = sent_tokenize(S)\n",
        "print(sentence_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIasco_JGdhI"
      },
      "source": [
        "<h3 id=\"Latihan:\"><font color=\"blue\">Latihan 1:</font></h3>\n",
        "\n",
        "<ul>\n",
        "\t<li>Apakah tanda baca seperti &quot;?&quot; atau &quot;!&quot; akan memisahkan kalimat?</li>\n",
        "\t<li>Apakah tanda &quot;carriage return&quot;/enter/ganti baris memisahkan kalimat?</li>\n",
        "\t<li>Apakah &quot;;&quot; memisahkan kalimat?</li>\n",
        "\t<li>Apakah tanda dash &quot;-&quot; memisahkan kata? Dalam bahasa Indonesia/Inggris?</li>\n",
        "</ul>\n",
        "\n",
        "<strong>Tips</strong>: Perhatikan bentuk <em>struktur data</em> &quot;output&quot; dari tokenisasi NLTK.<br />\n",
        "<strong>Catatan</strong>: pindah baris di Python string bisa dilakukan dengan menggunakan symbol &quot;\\n&quot;<br />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDJVkaTyzohF"
      },
      "source": [
        "## Jawaban Latihan 1\n",
        "\n",
        "1. Iya, Tanda tanya \"?\" dan tanda seru \"!\" biasanya dianggap sebagai pemisah kalimat dalam banyak algoritma tokenisasi, termasuk di NLTK. Jadi, kalimat yang diikuti dengan tanda tanya atau tanda seru akan dianggap sebagai pemisah antar kalimat.\n",
        "\n",
        "2. Kurang tepat, Tanda ini biasanya tidak dianggap sebagai pemisah kalimat oleh tokenizer default di NLTK. Namun, jika teks memiliki \"\\n\", itu akan dianggap sebagai pemisah baris atau paragraf.\n",
        "\n",
        "3. Titik koma \";\" tidak selalu dianggap sebagai pemisah kalimat dalam tokenisasi standar. Biasanya, ini lebih dipandang sebagai pemisah klausa atau bagian dalam kalimat yang lebih panjang.\n",
        "\n",
        "4. Tidak, Tanda dash \"-\" pada umumnya digunakan dalam kedua bahasa (Indonesia dan Inggris) untuk menghubungkan kata atau bagian kalimat, seperti dalam kata majemuk atau gabungan kata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GiakVvFRGdhK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', ',', 'Mr.', 'Man', '.', 'He', 'smiled', '!', '!', 'This', ',', 'i.e', '.', 'that', ',', 'is', 'it', '.']\n"
          ]
        }
      ],
      "source": [
        "S = \"Hello, Mr. Man. He smiled!! This, i.e. that, is it.\"\n",
        "word_token = word_tokenize(S)\n",
        "\n",
        "print(word_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello, Mr. Man.', 'He smiled!', '!', 'This ?', ', i.e.', 'that, is it.']\n"
          ]
        }
      ],
      "source": [
        "S = \"Hello, Mr. Man. He smiled! ! \\n This ? , i.e. that, is it.\"\n",
        "word_token = sent_tokenize(S)\n",
        "\n",
        "print(word_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4\n",
            "['Hello, Mr. Man.', 'He \\n smiled!!', 'This, i.e.', 'that, is it.']\n"
          ]
        }
      ],
      "source": [
        "S = \"Hello, Mr. Man. He \\n smiled!!  This, i.e. that, is it.\"\n",
        "word_token = sent_tokenize(S)\n",
        "\n",
        "print(len(word_token))\n",
        "print(word_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Ini kalimat pertama\\nIni kalimat kedua']\n",
            "Hasil dengan sent_tokenize:\n",
            "- Ini kalimat pertama\n",
            "Ini kalimat kedua\n"
          ]
        }
      ],
      "source": [
        "text = \"Ini kalimat pertama\\nIni kalimat kedua\"\n",
        "sentences = sent_tokenize(text)\n",
        "print(sentences)\n",
        "print(\"Hasil dengan sent_tokenize:\")\n",
        "for s in sentences:\n",
        "    print(\"-\", s)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['farhan;arya']"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = 'farhan;arya'\n",
        "sent_tokenize(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Dia', 'anak-anak', 'yang', 'well-known', ';', 'pintar', 'dan', 'rajin', '.']\n"
          ]
        }
      ],
      "source": [
        "kalimat = \"Dia anak-anak yang well-known; pintar dan rajin.\"\n",
        "tokens = word_tokenize(kalimat)\n",
        "print(tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKg5CsjqGdhL"
      },
      "source": [
        "## Tokenisasi dengan modul Spacy\n",
        "<strong>Kelebihan</strong>:</p>\n",
        "<ol>\n",
        "\t<li>Di claim lebih cepat (C-based)</li>\n",
        "\t<li>License termasuk untuk komersil</li>\n",
        "\t<li>Dukungan bahasa yang lebih banyak dari NLTK (termasuk bahasa Indonesia*)</li>\n",
        "</ol>\n",
        "\n",
        "<p><strong>Kekurangan</strong>:</p>\n",
        "<ol>\n",
        "\t<li>Fungsi yang lebih terbatas (dibandingkan NLTK).</li>\n",
        "\t<li>Karena berbasis compiler, sehingga instalasi cukup menantang.</li>\n",
        "</ol>\n",
        "\n",
        "<p><big><strong><a href=\"https://spacy.io/\" target=\"_blank\">https://spacy.io/</a></strong></big></p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "283fGuQtGdhN",
        "outputId": "8908beb7-783f-4710-f96b-a171b0105379"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', ',', 'Mr.', 'Man', '.', 'He', 'smiled', '!', '!', 'This', ',', 'i.e.', 'that', ',', 'is', 'it', '.']\n"
          ]
        }
      ],
      "source": [
        "# Contoh tokenisasi kata menggunakan Spacy\n",
        "import spacy\n",
        "\n",
        "# Loading language model\n",
        "\n",
        "spacy_en = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "S = \"Hello, Mr. Man. He smiled!! This, i.e. that, is it.\"\n",
        "tokens = spacy_en(S)\n",
        "print( [token.text for token in tokens] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "qDEfmuP6GdhO",
        "outputId": "b85c5fcf-31a3-4b21-bda5-4523a5d25d72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello, Mr. Man.', 'He smiled!!', 'This, i.e. that, is it.']\n"
          ]
        }
      ],
      "source": [
        "# Contoh tokenisasi kalimat dengan Spacy\n",
        "sentence_tokens = spacy_en(S).sents\n",
        "print([str(sent) for sent in sentence_tokens])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4o4De78GdhP"
      },
      "source": [
        "<h3 id=\"Latihan:\"><font color=\"blue\">Latihan 2:</font></h3>\n",
        "\n",
        "<ul>\n",
        "\t<li>Apakah hasil tokenisasi Spacy = NLTK? Mengapa?</li>\n",
        "\t<li>Lakukan latihan seperti yang dilakukan sebelumnya dengan modul NLTK, apakah hasilnya sama dengan Spacy?</li>\n",
        "</ul>\n",
        "\n",
        "<p><strong>Tips</strong>: Perhatikan bentuk <em>struktur data</em> &quot;output&quot; dari tokenisasi Spacy juga berbeda dengan NLTK.<br />\n",
        "<strong>Catatan</strong>: Contoh sederhana ini menekankan perbedaan ilmu linguistik dan computational linguistic.</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1TAgGGaGdhQ",
        "outputId": "52754a7f-2ce1-45c5-e2c1-5d8fced979d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', ',', 'Mr.', 'Man', '.', 'He', 'smiled', '!', '!', 'This', ',', 'i.e', '.', 'that', ',', 'is', 'it', '.']\n"
          ]
        }
      ],
      "source": [
        "# Kerjakan latihan 2 pada cell berikut ini\n",
        "# untuk NLTK\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "S = \"Hello, Mr. Man. He smiled!! This, i.e. that, is it.\"\n",
        "\n",
        "# Tokenisasi kata\n",
        "tokens = word_tokenize(S)\n",
        "\n",
        "# Menampilkan hasil tokenisasi\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wj7s8kGt38rB",
        "outputId": "ffd8c9c7-be53-4474-9f71-5678d25b3848"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello, Mr. Man.', 'He smiled!!', 'This, i.e.', 'that, is it.']\n"
          ]
        }
      ],
      "source": [
        "sentence_tokens_nltk = sent_tokenize(S)\n",
        "\n",
        "# Menampilkan hasil tokenisasi kalimat\n",
        "print(sentence_tokens_nltk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64QHMGQv2x1A"
      },
      "source": [
        "## Jawaban Latihan 2\n",
        "\n",
        "1. Hasil dari tokenize menggunakan NLTK hampir sama dengan Spacy, tetapi ada sedikit perbedaan di kata ('i.e.') pada spacy kata ('i.e.) dianggap menjadi kesatuan sedangkan di NLTK itu dipisah menjadi ('i.e') dan ('.'). Karena Spacy memiliki model yang lebih canggih dan memanfaatkan konteks, jadi ia tahu bahwa \"i.e.\" adalah singkatan yang harus diperlakukan sebagai satu token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUv9p0QJGdhR"
      },
      "source": [
        "## Tokenisasi dengan TextBlob\n",
        "<strong>Kelebihan</strong>:</p>\n",
        "<ol>\n",
        "\t<li>Sederhana &amp; mudah untuk digunakan/pelajari.</li>\n",
        "\t<li>Textblob objects punya behaviour/properties yang sama dengan string di Python.</li>\n",
        "\t<li>TextBlob dibangun dari kombinasi modul NLTK dan (Clips) Pattern</li>\n",
        "</ol>\n",
        "\n",
        "<p><strong>Kekurangan</strong>:</p>\n",
        "<ol>\n",
        "\t<li>Tidak secepat Spacy dan NLTK</li>\n",
        "\t<li>Language Model terbatas: English, German, French</li>\n",
        "</ol>\n",
        "\n",
        "<p>*Blob : Binary large Object</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_Ko97wKGdhS",
        "outputId": "6b7d7f6d-bdfb-4962-fb00-35ebb07f6e96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', 'Mr', 'Man', 'He', 'smiled', 'This', 'i.e', 'that', 'is', 'it']\n",
            "['Hello, Mr. Man.', 'He smiled!!', 'This, i.e.', 'that, is it.']\n"
          ]
        }
      ],
      "source": [
        "# Contoh tokenisasi dengan TextBlob\n",
        "from textblob import TextBlob\n",
        "\n",
        "T = \"Hello, Mr. Man. He smiled!! This, i.e. that, is it.\"\n",
        "sentence_tokens = TextBlob(T).sentences\n",
        "\n",
        "# Tokenisasi kata\n",
        "print(TextBlob(T).words)\n",
        "\n",
        "# Tokenisasi kalimat\n",
        "print([str(sent) for sent in sentence_tokens])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgyUgtWZGdhT"
      },
      "source": [
        "<h3 id=\"Latihan:\"><font color=\"blue\">Latihan 3:</font></h3>\n",
        "\n",
        "<ul>\n",
        "\t<li>Ada yang berbeda dari hasilnya?&nbsp;Apakah lebih baik seperti ini?</li>\n",
        "</ul>\n",
        "\n",
        "<p><strong>Tips</strong>: TextBlob biasa digunakan untuk prototyping pada data yang tidak terlalu besar.<br />\n",
        "<strong>Catatan</strong>: Hati-hati tipe data Blob tidak biasa (objek).</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WA8Xoqi50Pc"
      },
      "source": [
        "## Jawaban Latihan 3\n",
        "\n",
        "1. Hasilnya menunjukkan bahwa hasil dari NLTK dan Text blob sama, berbeda dengan Spacy, menurut saya Jika tujuan kita adalah untuk menangani teks yang lebih alami dan akurat dalam konteks kalimat, Spacy adalah pilihan yang lebih baik karena ia lebih pintar dalam menangani konteks semantik, seperti singkatan \"i.e.\". Namun, NLTK dan TextBlob mungkin lebih berguna untuk tugas yang lebih sederhana"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6O1a7rfGdhU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dengan textblob:\n",
            "['Hello, Mr. Man.', 'He smiled!!', 'This, i.e.', 'that, is it.']\n",
            "\n",
            "Dengan spacy\n",
            "['Hello', ',', 'Mr.', 'Man', '.', 'He', 'smiled', '!', '!', 'This', ',', 'i.e.', 'that', ',', 'is', 'it', '.']\n",
            "\n",
            "Dengan nltk\n",
            "['Hello', ',', 'Mr.', 'Man', '.', 'He', 'smiled', '!', '!', 'This', ',', 'i.e', '.', 'that', ',', 'is', 'it', '.']\n"
          ]
        }
      ],
      "source": [
        "# Kerjakan latihan 3 pada cell berikut ini\n",
        "print('Dengan textblob:')\n",
        "print(TextBlob(T).words)\n",
        "print()\n",
        "print('Dengan spacy')\n",
        "print([token.text for token in spacy_en(T)])\n",
        "print()\n",
        "print('Dengan nltk')\n",
        "print(word_tokenize(T))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdwNjoHbGdhV"
      },
      "source": [
        "## Tokenisasi: language dependent dan environment dependent\n",
        "\n",
        "<p>Tokenization sebenarnya tidak sesederhana memisahkan berdasarkan spasi dan removing symbol. Sebagai contoh dalam bahasa Jepang/Cina/Arab suatu kata bisa terdiri dari beberapa karakter.</p>\n",
        "\n",
        "<p><img alt=\"\" src=\"figures/2_Tokenization_Complexity.jpg\" style=\"height:500px; width:686px\" /><br />\n",
        "[<a href=\"http://aclweb.org/anthology/Y/Y11/Y11-1038.pdf\" target=\"_blank\"><strong>Image Source</strong></a>]</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDkd2CQkGdhW"
      },
      "source": [
        "__Bagaimana untuk data yang punya karakteristik tertentu seperti Twitter?__\n",
        "__Apakah bisa menggunakan modul tokenisasi yang telah tersedia?__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHpcLCQnGdhX",
        "outputId": "96e445a1-9a43-4f34-bc5f-a06d707a223b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['I', 'am', 'so', 'happpy', ',', 'supeeer', 'happpy', ':)', '#imsohappy', '#happy']\n"
          ]
        }
      ],
      "source": [
        "# Contoh Tokenizer untuk twitter\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "Tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
        "tweet = \"@text_mining I am so happpppppppy, supeeeer happpy :) #imsohappy #happy\"\n",
        "print(Tokenizer.tokenize(tweet))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PR8qjXXVGdhY",
        "outputId": "ddcc962d-5457-4b38-eb48-a5a978a7782d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "@text_mining I am so super\n"
          ]
        }
      ],
      "source": [
        "import itertools\n",
        "\n",
        "tweet = \"@text_mining I am so supeeeeeeer\"\n",
        "\n",
        "# Menghilangkan double karakter\n",
        "tweet_clear = ''.join(''.join(s)[:1] for _, s in itertools.groupby(tweet))\n",
        "print(tweet_clear)\n",
        "\n",
        "# NOTES: untuk beberapa data spesifik seperti data bioinformatics, cryptography,\n",
        "# twitter, dst dibutuhkan tokenizer custom untuk dapat memenuhi kebutuhan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrF6c6VgGdhZ"
      },
      "source": [
        "## NLP Bahasa Indonesia:\n",
        "\n",
        "<p>module NLP Python yang support bahasa Indonesia secara umum masih sangat langka. Beberapa <u><strong>resources </strong></u>yang dapat digunakan:</p>\n",
        "\n",
        "<ol>\n",
        "\t<li><strong><a href=\"https://github.com/kirralabs/indonesian-NLP-resources\" target=\"_blank\">KirraLabs</a></strong>: Mix of NLP-TextMining resources</li>\n",
        "\t<li><strong><a href=\"https://pypi.python.org/pypi/Sastrawi/1.0.1\" target=\"_blank\">Sastrawi 1.0.1</a>:</strong>&nbsp;untuk &quot;stemming&quot; &amp;&nbsp;<strong><a href=\"https://devtrik.com/python/stopword-removal-bahasa-indonesia-python-sastrawi/\" target=\"_blank\">stopwords&nbsp;</a></strong>bahasa Indonesia.</li>\n",
        "\t<li><strong><a href=\"http://stop-words-list-bahasa-indonesia.blogspot.co.id/2012/09/daftar-kata-dasar-bahasa-indonesia.html\" target=\"_blank\">Daftar Kata Dasar Indonesia</a></strong>:&nbsp;Bisa di load sebagai dictionary di Python</li>\n",
        "\t<li><strong><a href=\"https://id.wiktionary.org/wiki/Wiktionary:ProyekWiki_bahasa_Indonesia/Daftar_kata\" target=\"_blank\">Wiktionary</a></strong>: ProyekWiki bahasa Indonesia [termasuk Lexicon]</li>\n",
        "\t<li><a href=\"http://wn-msa.sourceforge.net/\" target=\"_blank\"><strong>WordNet Bahasa Indonesia</strong></a>: Bisa di load&nbsp;sebagai dictionary (atau NLTK<em>*</em>) di Python.</li>\n",
        "\t<li><strong><a href=\"http://kakakpintar.com/daftar-kata-baku-dan-tidak-baku-a-z-dalam-bahasa-indonesia/\" target=\"_blank\">Daftar Kata Baku-Tidak Baku</a></strong>: Bisa di load sebagai dictionary di Python.</li>\n",
        "\t<li><strong><a href=\"https://spacy.io/\" target=\"_blank\">Spacy</a></strong>: Cepat/efisien, MIT License, tapi language model Indonesia masih terbatas.</li>\n",
        "\t<li><a href=\"http://ufal.mff.cuni.cz/udpipe\" target=\"_blank\"><strong>UdPipe</strong></a>: Online request &amp; restricted license (support berbagai bahasa -&nbsp;pemrograman).</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3HzeEHbGdhc",
        "outputId": "305c44b9-e22f-4e63-deaf-dcacabab5413"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Sore', 'itu', ',', 'Hamzah', 'melihat', 'kupu-kupu', 'di', 'taman', '.', 'Ibu', 'membeli', 'oleh-oleh', 'di', 'pasar']\n"
          ]
        }
      ],
      "source": [
        "# Contoh Tokenisasi dalam bahasa Indonesia dengan Spacy\n",
        "from spacy.lang.id import Indonesian\n",
        "from spacy.lang.en import English\n",
        "\n",
        "# load language model bahasa Indonesia\n",
        "spacy_id = Indonesian()\n",
        "spacy_en = English()\n",
        "\n",
        "S = 'Sore itu, Hamzah melihat kupu-kupu di taman. Ibu membeli oleh-oleh di pasar'\n",
        "word_token = spacy_id(S)\n",
        "print([token.text for token in word_token])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYJUtQDmGdhd",
        "outputId": "26569369-b7c8-4d88-e2e2-87c39bba8776"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Sore', 'itu', ',', 'Hamzah', 'melihat', 'kupu', '-', 'kupu', 'di', 'taman', '.', 'Ibu', 'membeli', 'oleh', '-', 'oleh', 'di', 'pasar']\n"
          ]
        }
      ],
      "source": [
        "# Jika menggunakan Language model English:\n",
        "word_token_en = spacy_en(S)\n",
        "print([token.text for token in word_token_en])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTdYrRpxGdhe"
      },
      "source": [
        "<h3 id=\"Latihan:\"><font color=\"blue\">Latihan 4:</font></h3>\n",
        "\n",
        "<ul>\n",
        "\t<li>Apakah ada perbedaan apabila menggunakan language model yang berbeda?</li>\n",
        "    <li>Bagaimana jika melakukan tokenisasi Bahasa Indonesia dengan NLTK? Apakah hasilnya sama dengan Spacy?\n",
        "</ul>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenisasi dengan NLTK:\n",
            "['Sore', 'itu', ',', 'Hamzah', 'melihat', 'kupu-kupu', 'di', 'taman', '.', 'Ibu', 'membeli', 'oleh-oleh', 'di', 'pasar']\n",
            "\n",
            "Tokenisasi dengan Spacy:\n",
            "['Sore', 'itu', ',', 'Hamzah', 'melihat', 'kupu-kupu', 'di', 'taman', '.', 'Ibu', 'membeli', 'oleh-oleh', 'di', 'pasar']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Tokenisasi Bahasa Indonesia dengan NLTK\n",
        "\n",
        "# Menggunakan variabel S yang sudah ada\n",
        "tokens_nltk = word_tokenize(S)\n",
        "\n",
        "# Menampilkan hasil tokenisasi dengan NLTK\n",
        "print(\"Tokenisasi dengan NLTK:\")\n",
        "print(tokens_nltk)\n",
        "\n",
        "# Tokenisasi Bahasa Indonesia dengan Spacy\n",
        "tokens_spacy = [token.text for token in spacy_id(S)]\n",
        "\n",
        "# Menampilkan hasil tokenisasi dengan Spacy\n",
        "print(\"\\nTokenisasi dengan Spacy:\")\n",
        "print(tokens_spacy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX9v2MXJ7Gra"
      },
      "source": [
        "## Jawaban Latihan 4\n",
        "\n",
        "1. iya, perbedaan dalam penggunaan language model pada Spacy untuk bahasa Indonesia dan bahasa Inggris bisa menghasilkan hasil tokenisasi yang berbeda. misal spacy_id itu untuk indonesia sedangkan spacy_en itu untuk bahasa inggris\n",
        "\n",
        "2. Untuk perbedaan tidak ada kedunya mampu memisah kalimat majemuk seperti kupu-kupu dan oleh oleh dengan baik, dari NLTK maupun Spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHcd3fiSGdhg"
      },
      "source": [
        "## Casefolding\n",
        "\n",
        "<p> Casefolding dilakukan untuk merubah karakter ke dalam huruf besar (uppercase) atau huruf kecil (lowercase) </p>\n",
        "\n",
        "<ul>\n",
        "\t<li>Untuk menganalisa makna (<em>semantic</em>) dari suatu (frase) kata dan mencari informasi dalam proses textmining, seringnya kita tidak membutuhkan informasi huruf besar/kecil dari kata&nbsp;tersebut.</li>\n",
        "    <li>Casefolding dapat dilakukan dengan efisien tanpa melalui proses tokenisasi</li>\n",
        "\t<li>Namun, bergantung pada analisa teks yang akan digunakan pengguna harus berhati-hati dengan urutan proses (pipelining) dalam preprocessing. </li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Eo1d58WGdhg",
        "outputId": "df1609c0-6a36-46aa-fb70-07303433995d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hi there!, i am a student. nice to meet you :)\n",
            "HI THERE!, I AM A STUDENT. NICE TO MEET YOU :)\n"
          ]
        }
      ],
      "source": [
        "# Contoh casefolding\n",
        "S = \"Hi there!, I am a student. Nice to meet you :)\"\n",
        "print(S.lower())\n",
        "print(S.upper())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rog6T5AeGdhh"
      },
      "source": [
        "<h3 id=\"Latihan:\"><font color=\"blue\">Latihan 5:</font></h3>\n",
        "\n",
        "<ul>\n",
        "\t<li>Temukan minimal 2 pengecualian dimana huruf besar dan kecil mempengaruhi makna dalam pemrosesan teks</li>\n",
        "    <li>Mengapa casefolding dapat dilakukan secara efisien tanpa melalui tahap tokenisasi?</li>\n",
        "    <li>Berikan contoh pengaruh dari urutan proses dalam preprocessing yang berpengaruh terhadap hasil preprocessing </li>\n",
        "        \n",
        "</ul>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SR2ETva77ui"
      },
      "source": [
        "## Jawaban Latihan 5\n",
        "\n",
        "1. Nama atau singkatan khusus\n",
        "ex = us vs US -->  \"us\" dapat merujuk pada kata ganti orang pertama \"kami\" dalam bahasa Inggris, sedangkan \"US\" adalah singkatan dari \"United States\" (Amerika Serikat).\n",
        "\n",
        "Akronim atau singkatan\n",
        "contoh : \"HIV\" vs \"hiv\":\n",
        "\"HIV\" adalah singkatan dari Human Immunodeficiency Virus, virus yang menyebabkan AIDS.\n",
        "\"hiv\" dengan huruf kecil bisa dianggap sebagai kesalahan atau tidak dikenali sebagai akronim yang valid.\n",
        "\n",
        "2. Casefolding dapat dilakukan secara efisien tanpa melalui tahap tokenisasi karena casefolding berfungsi pada level karakter dalam teks, sedangkan tokenisasi berfokus pada pemisahan teks menjadi unit-unit yang lebih besar, seperti kata atau kalimat.\n",
        "\n",
        "3. Urutan dalam preprocessing teks sangat mempengaruhi hasil akhir. Beberapa langkah, seperti casefolding, stopwords removal, lemmatization, dan menghapus tanda baca, jika dilakukan secara berantakan misalnya stemming duluan kita bakal kehilangan berbagai informasi yang terdapat pada text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Contoh 1: us vs US\n",
            "Contoh 2: Apple vs apple\n",
            "Casefolded text: hello, mr. man. he smiled!!\n",
            "Stemmed before casefolding: run\n",
            "Stemmed after casefolding: run\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Jawaban untuk latihan\n",
        "\n",
        "# 1. Pengecualian huruf besar dan kecil mempengaruhi makna:\n",
        "# Contoh 1: \"us\" vs \"US\"\n",
        "example_1 = \"us\"  # Kata ganti orang pertama jamak\n",
        "example_2 = \"US\"  # Singkatan dari United States\n",
        "print(f\"Contoh 1: {example_1} vs {example_2}\")\n",
        "\n",
        "# Contoh 2: \"Apple\" vs \"apple\"\n",
        "example_3 = \"Apple\"  # Nama perusahaan teknologi\n",
        "example_4 = \"apple\"  # Nama buah\n",
        "print(f\"Contoh 2: {example_3} vs {example_4}\")\n",
        "\n",
        "# 2. Mengapa casefolding dapat dilakukan secara efisien tanpa melalui tahap tokenisasi?\n",
        "# Casefolding bekerja pada level karakter, sehingga tidak memerlukan pemisahan teks menjadi token terlebih dahulu.\n",
        "# Contoh:\n",
        "text = \"Hello, Mr. Man. He smiled!!\"\n",
        "casefolded_text = text.lower()\n",
        "print(f\"Casefolded text: {casefolded_text}\")\n",
        "\n",
        "# 3. Contoh pengaruh urutan proses dalam preprocessing:\n",
        "# Jika stemming dilakukan sebelum casefolding, hasilnya bisa tidak konsisten.\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "text = \"Running\"\n",
        "stemmed_before_casefolding = stemmer.stem(text)\n",
        "stemmed_after_casefolding = stemmer.stem(text.lower())\n",
        "\n",
        "print(f\"Stemmed before casefolding: {stemmed_before_casefolding}\")\n",
        "print(f\"Stemmed after casefolding: {stemmed_after_casefolding}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FwO1Ta-Gdhi"
      },
      "source": [
        "## Stemming dan Lemmatization\n",
        "\n",
        "<ol>\n",
        "\t<li>\n",
        "\t<p><strong>Stemmer</strong>&nbsp;akan menghasilkan sebuah bentuk kata yang disepakati oleh suatu sistem tanpa mengindahkan konteks kalimat. Syaratnya beberapa kata dengan makna serupa hanya perlu dipetakan secara konsisten ke sebuah kata baku.&nbsp;Banyak digunakan di IR &amp;&nbsp;komputasinya relatif sedikit. Biasanya dilakukan dengan menghilangkan imbuhan (suffix/prefix).</p>\n",
        "\t</li>\n",
        "\t<li>\n",
        "\t<p><strong>lemmatisation</strong> akan menghasilkan kata baku (dictionary word) dan bergantung konteks.</p>\n",
        "\t</li>\n",
        "\t<li>\n",
        "\t<p>Lemma &amp; stemming bisa jadi sama-sama menghasilkan suatu akar kata (root word). Misal : <em>Melompat </em>==&gt; <em>lompat</em></p>\n",
        "\t</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spXPmefnGdhi"
      },
      "source": [
        "<p><strong>Mengapa melakukan Stemming &amp; Lemmatisasi</strong>?</p>\n",
        "\n",
        "<ol>\n",
        "\t<li>Sering digunakan di IR (Information Retrieval) agar ketika seseorang mencari kata tertentu, maka seluruh kata yang terkait juga diikutsertakan.<br />\n",
        "\tMisal:&nbsp;<em>organize</em>,&nbsp;<em>organizes</em>, and&nbsp;<em>organizing&nbsp;</em>&nbsp;dan&nbsp;<em>democracy</em>,&nbsp;<em>democratic</em>, and&nbsp;<em>democratization</em>.</li>\n",
        "\t<li>Di Text Mining Stemming dan Lemmatisasi akan mengurangi dimensi (mengurangi variasi morphologi), yang terkadang akan meningkatkan akurasi.</li>\n",
        "\t<li>Tapi di IR efeknya malah berkebalikan: <strong><font color=\"blue\">meningkatkan recall, tapi menurunkan akurasi&nbsp;</font></strong>[<a href=\"https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\" target=\"_blank\"><strong>Link</strong></a>]. Contoh: kata&nbsp;<em>operate, operating, operates, operation, operative, operatives, dan operational</em>&nbsp;jika di stem menjadi <em>operate</em>, maka ketika seseorang mencari &quot;<em>operating system</em>&quot;, maka entry seperti&nbsp;<em>operational and research</em> dan&nbsp;<em>operative and dentistry</em>&nbsp;akan muncul sebagai entry dengan relevansi yang cukup tinggi.</li>\n",
        "</ol>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2fX_F8VGdhi"
      },
      "source": [
        "__Stemming tidak perlu \"benar\", hanya perlu konsisten__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW62Po_kGdhj"
      },
      "source": [
        "## NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "rToCTObtGdhj",
        "outputId": "33c2c7c5-fdbf-4fd9-fd3a-dcf8c0d55a9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence:  presumably I would like to MultiPly my provision, saying tHat without crYing\n",
            "Lancaster :  presum i would lik to multiply my provision, say that without cry\n",
            "Porter :  presum i would like to multipli my provision, say that without cri\n",
            "SnowBall :  presum i would like to multipli my provision, say that without cri\n"
          ]
        }
      ],
      "source": [
        "# Contoh Stemming di NLTK\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "S = 'presumably I would like to MultiPly my provision, saying tHat without crYing'\n",
        "print('Sentence: ',S)\n",
        "\n",
        "stemmer_list = [LancasterStemmer, PorterStemmer, SnowballStemmer]\n",
        "names = ['Lancaster', 'Porter', 'SnowBall']\n",
        "for stemmer_name,stem in zip(names,stemmer_list):\n",
        "    if stemmer_name == 'SnowBall':\n",
        "        st = stem('english')\n",
        "    else:\n",
        "        st = stem()\n",
        "    print(stemmer_name,': ',' '.join(st.stem(s) for s in S.split()))\n",
        "# perhatikan, kita tidak melakukan case normalization (lowercase)\n",
        "# Hasil stemming bisa tidak bermakna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ww2jyHXqGdhk",
        "outputId": "68eb18f7-61fa-40cd-e3ea-11326e51640f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence:  apples and oranges are similar. boots and hippos aren't, don't you think?\n",
            "Lemmatize:  apple and orange are similar. boot and hippo aren't, don't you think?\n"
          ]
        }
      ],
      "source": [
        "# Contoh Lemmatizer di NLTK\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "S = \"apples and oranges are similar. boots and hippos aren't, don't you think?\"\n",
        "print('Sentence: ', S)\n",
        "print('Lemmatize: ',' '.join(lemmatizer.lemmatize(s) for s in S.split()))\n",
        "# Lemma case sensitive. Dengan kata lain string harus diubah ke dalam bentuk huruf kecil (lower case)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPZ1opHHGdhl",
        "outputId": "6f8a92d6-f159-49dd-888a-fe7671bb9015"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "good\n",
            "better\n"
          ]
        }
      ],
      "source": [
        "# Lemmatizer menggunakan informasi pos. \"pos\" (part-of-speech) akan dibahas di segmen berikutnya\n",
        "print(lemmatizer.lemmatize(\"better\", pos=\"a\")) # adjective\n",
        "print(lemmatizer.lemmatize(\"better\", pos=\"v\")) # verb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ISmjsdMGdhl"
      },
      "source": [
        "## TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaZa3o97Gdhm",
        "outputId": "a9c67489-dac1-4247-bf31-63023ab94c2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stem:  run\n",
            "Lemmatize:  go\n"
          ]
        }
      ],
      "source": [
        "# Contoh TextBlob Stemming & Lemmatizer\n",
        "from textblob import Word\n",
        "# Stemming\n",
        "print(\"Stem: \", Word('running').stem()) # menggunakan NLTK Porter stemmer\n",
        "\n",
        "# Lemmatizer\n",
        "print(\"Lemmatize: \", Word('went').lemmatize('v'))\n",
        "\n",
        "# default Noun, plural akan menjadi singular dari akar katanya\n",
        "# Juga case sensitive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-GaTcnsGdhm"
      },
      "source": [
        "## Spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sd2Oa6XWGdhn",
        "outputId": "0cfeda7f-7d3d-4788-f018-9a051a2511bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-PRON- be sure apples and orange be similar . boot and hippo be not , do not -PRON- think ?\n"
          ]
        }
      ],
      "source": [
        "# Spacy Lemmatizer English\n",
        "sent = \"I am sure Apples and oranges are similar. Boots and hippos aren't, don't you think?\"\n",
        "sent_token = spacy_en(sent)\n",
        "print( ' '.join( s.lemma_ for s in sent_token ) )\n",
        "# HATI-HATI dengan lemma \"I\" dan \"you\" di Spacy!!!..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVqT805NGdho",
        "outputId": "4e82e327-393e-4013-806d-1032cda196f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "raya itu bareng dengan saat kita pergi ke Jogjakarta\n"
          ]
        }
      ],
      "source": [
        "# Spacy Lemmatizer Indonesia\n",
        "I = \"perayaan itu berbarengan dengan saat kita bepergian ke Jogjakarta\"\n",
        "idn = spacy_id(I)\n",
        "print( ' '.join( k.lemma_ for k in idn ) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTX9OEwhGdho",
        "outputId": "1c6c15cc-0a3f-4dec-cf26-2d736769508e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Perayaan', 'Bepergian']\n"
          ]
        }
      ],
      "source": [
        "# Perhatikan output berikut (hati-hati inkonsistensi)\n",
        "print([k.lemma_ for k in spacy_id(\"Perayaan Bepergian\")])\n",
        "\n",
        "# bagaimana dengan Spacy stemmer??\n",
        "# Spacy belum support stemmer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwunTiGrGdhp"
      },
      "source": [
        "## Sastrawi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IuF7tEOGdhq",
        "outputId": "9433e372-9aee-4ff2-d76d-f3cb953eb82a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "raya itu bareng dengan saat kita pergi ke makassar\n",
            "raya pergi suara\n"
          ]
        }
      ],
      "source": [
        "# Lemmatizer dengan Sastrawi\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "stemmer = StemmerFactory().create_stemmer()\n",
        "\n",
        "I = \"perayaan itu berbarengan dengan saat kita bepergian ke Makassar\"\n",
        "print(stemmer.stem(I))\n",
        "print(stemmer.stem(\"Perayaan Bepergian Menyuarakan\"))\n",
        "# Ada beberapa hal yang berbeda antara Sastrawi dan modul-modul diatas.\n",
        "# Apa sajakah?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAuO4vnPGdhr"
      },
      "source": [
        "<h3 id=\"Tips:\">Tips:</h3>\n",
        "\n",
        "<ul>\n",
        "\t<li>Secara umum &#39;biasanya&#39; di Text Mining yang kita butuhkan hanyalah <strong><font color=\"blue\">Lemma</font></strong>.</li>\n",
        "\t<li>&quot;Kecuali&quot; di aplikasi IR, spelling correction, variasi kata, clustering, atau terkadang klasifikasi. Pada aplikasi-aplikasi tersebut stemming terkadang lebih diinginkan.</li>\n",
        "\t<li>Stemming jauh lebih cepat, tapi tidak selalu tersedia di modul NLP.</li>\n",
        "\t<li>Beberapa algoritma tertentu membutuhkan tanda &quot;.&quot; dan &quot;,&quot; : contohnya untuk document summarization di textRank.</li>\n",
        "\t<li>&quot;_&quot; juga biasa digunakan untuk menyatakan frase kata di representasi n-grams (misal &quot;buah_tangan&quot;).</li>\n",
        "\t<li>Stemming juga digunakan pada Word Sense Disambiguation (WSD)</li>\n",
        "</ul>\n",
        "\n",
        "<h3 id=\"Diskusi:\">Diskusi:</h3>\n",
        "\n",
        "<ul>\n",
        "\t<li>Untuk menghemat storage database, apakah sebaiknya kita menyimpan saja hasil preprocessed texts/documents?</li>\n",
        "</ul>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVwGUT8iGdhr"
      },
      "source": [
        "## Part-of-Speech (POS) tag\n",
        "\n",
        "<p> POS tagging merupakan proses pemberian tanda berupa kelas kata pada setiap kata yang terdapat di dalam corpus.</p>\n",
        "\n",
        "<p><img alt=\"\" src=\"figures/2_parts-of-speech-chart.jpg\" style=\"height:400px; width:404px\" /></p>\n",
        "<p>[<a href=\"https://www.paperrater.com/page/parts-of-speech\" target=\"_blank\">image source</a>]</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     C:\\Users\\farha\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('averaged_perceptron_tagger_eng')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        },
        "id": "YlAoEM0fGdhs",
        "outputId": "787b2a9c-ef06-4829-d83c-b0018f174aa4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('I', 'PRP'), ('am', 'VBP'), ('currently', 'RB'), ('learning', 'VBG'), ('NLP', 'NNP'), ('in', 'IN'), ('English', 'NNP'), (',', ','), ('but', 'CC'), ('if', 'IN'), ('possible', 'JJ'), ('I', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('know', 'VB'), ('NLP', 'NNP'), ('in', 'IN'), ('Indonesian', 'JJ'), ('language', 'NN'), ('too', 'RB')]\n"
          ]
        }
      ],
      "source": [
        "# Contoh POS tags dengan NLTK (bahasa Inggris)\n",
        "from nltk import pos_tag\n",
        "S = 'I am currently learning NLP in English, but if possible I want to know NLP in Indonesian language too'\n",
        "\n",
        "tokens = word_tokenize(S)\n",
        "print(pos_tag(tokens))\n",
        "# Tidak lagi hanya 9 macam tags seperti yang dibahas ahli bahasa (linguist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmnZ1JmFGdht"
      },
      "source": [
        "### Daftar POS tag NLTK:\n",
        "\n",
        "<img alt=\"\" src=\"figures/2_post_tags_NLTK.png\" style=\"height:400px; width:516px\" /></h3>\n",
        "\n",
        "<p>[<a href=\"http://gitqwerty777.github.io/natural-language-processing/\" target=\"_blank\">image source</a>]</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 928
        },
        "id": "dasYYZLLGdht",
        "outputId": "f2fa4bc6-c1e4-41eb-e11b-6e313584f0fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello NNP, Mr. NNP, Man NNP, He PRP, smiled VBD, This DT, i.e NN, that DT, is VBZ, it PRP, "
          ]
        }
      ],
      "source": [
        "# Contoh POS tag dengan TextBlob pada bahasa Inggris\n",
        "for word, pos in TextBlob(T).tags:\n",
        "    print(word, pos, end=', ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tz5jRrXyGdhu",
        "outputId": "df0f43c3-fe9a-4bfb-a246-8c2ffc3df23e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello UH, , ,, Mr. NNP, Man NNP, . ., He PRP, smiled VBD, ! ., ! ., This DT, , ,, i.e. FW, that DT, , ,, is VBZ, it PRP, . ., "
          ]
        }
      ],
      "source": [
        "# Contoh POS tag dengan Spacy pada bahasa Inggris\n",
        "tokens = spacy_en(T)\n",
        "for token in tokens:\n",
        "    print(token,token.tag_, end =', ')\n",
        "\n",
        "# Spacy belum support POS tag untuk bahasa Indonesia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yOD2-8tGdhv",
        "outputId": "35c45dcf-7251-4589-b93b-5daceef61471"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'adverb'"
            ]
          },
          "execution_count": 29,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Untuk mengetahui arti dari POS tag pada Spacy dapat menggunakan perintah \"explain\"\n",
        "spacy.explain('RB')\n",
        "# Daftar Lengkap: https://spacy.io/api/annotation#pos-tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gi-9cierGdhw",
        "outputId": "40f09d46-ee92-4bd6-e610-09acbe809d45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[('Saya', 'PRP'), ('bekerja', 'VB'), ('di', 'IN'), ('Bandung', 'NNP')]]\n"
          ]
        }
      ],
      "source": [
        "# POS tag bahasa Indonesia dengan NLTK\n",
        "# https://yudiwbs.wordpress.com/2018/02/20/pos-tagger-bahasa-indonesia-dengan-pytho/\n",
        "\n",
        "from nltk.tag import CRFTagger\n",
        "ct = CRFTagger()\n",
        "ct.set_model_file('data/all_indo_man_tag_corpus_model.crf.tagger')\n",
        "\n",
        "hasil = ct.tag_sents([['Saya','bekerja','di','Bandung']])\n",
        "print(hasil)\n",
        "# Hati-hati dengan struktur data inputnya"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTjcdke8Gdhw"
      },
      "source": [
        "<h3 id=\"Latihan:\"><font color=\"blue\">Latihan 6:</font></h3>\n",
        "\n",
        "<ul>\n",
        "\t<li>Kapan harus melakukan POS tagging pada tahap preprocessing?</li>\n",
        "    <li>Buatlah contoh hasi dari POS tag dengan hanya mengambil kata yang memiliki tag NOUN (*), dan berikan contoh kasus penggunaannya?</li>\n",
        "    <li>Buatlah contoh hasil dari POS tag yang telah ditambahkan pada setiap kata dalam suatu kalimat dengan menggunakan NLTK (**)</li>\n",
        "        \n",
        "</ul>\n",
        "\n",
        "<p>(*) <strong>Input</strong>: \"The tiger (Panthera tigris) is the largest extant cat species and a member of the genus Panthera. It is most recognisable for its dark vertical stripes on orange-brown fur with a lighter underside. It is an apex predator, primarily preying on ungulates such as deer and wild boar.\"</p>\n",
        "<p>(**) <strong>Input</strong>: \"I am currently learning NLP in English, but if possible I want to know NLP in Indonesian language too\"</p>\n",
        "<p> <strong>Expected output</strong>: \"I_PRP am_VBP currently_RB learning_VBG NLP_NNP in_IN English_NNP ,_, but_CC if_IN possible_JJ I_PRP want_VBP to_TO know_VB NLP_NNP in_IN Indonesian_JJ language_NN too_RB\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjQFUz2G-YVG"
      },
      "source": [
        "## Jawaban Latihan 6\n",
        "\n",
        "1. POS (Part-of-Speech) tagging biasanya dilakukan setelah beberapa langkah preprocessing dasar seperti tokenization, case folding, dan penghapusan stopwords. POS tagging penting untuk menganalisis struktur gramatikal teks dan mengidentifikasi kata berdasarkan kategorinya (misalnya, kata benda, kata kerja, kata sifat, dll.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tk6gqjD1Gdhx",
        "outputId": "d9260443-d7fc-4060-ea7d-0c3d03c08c8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['tiger', 'Panthera', 'tigris', 'cat', 'species', 'member', 'genus', 'Panthera', 'stripes', 'fur', 'underside', 'predator', 'ungulates', 'deer', 'boar']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     C:\\Users\\farha\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "# Kerjakan latihan 6 pada cell berikut ini\n",
        "# No 2\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "# Kalimat input\n",
        "sentence = \"The tiger (Panthera tigris) is the largest extant cat species and a member of the genus Panthera. It is most recognisable for its dark vertical stripes on orange-brown fur with a lighter underside. It is an apex predator, primarily preying on ungulates such as deer and wild boar.\"\n",
        "\n",
        "# Tokenisasi dan POS tagging\n",
        "tokens = word_tokenize(sentence)\n",
        "tagged = pos_tag(tokens)\n",
        "\n",
        "# Filter kata yang memiliki tag NOUN\n",
        "nouns = [word for word, tag in tagged if tag in ['NN', 'NNS', 'NNP', 'NNPS']]\n",
        "\n",
        "# Menampilkan hasil\n",
        "print(nouns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ombY7FBH_Qe4",
        "outputId": "fd161493-1ad2-49ef-e7cf-097bb2a1dce1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('I', 'PRP'), ('am', 'VBP'), ('currently', 'RB'), ('learning', 'VBG'), ('NLP', 'NNP'), ('in', 'IN'), ('English', 'NNP'), (',', ','), ('but', 'CC'), ('if', 'IN'), ('possible', 'JJ'), ('I', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('know', 'VB'), ('NLP', 'NNP'), ('in', 'IN'), ('Indonesian', 'JJ'), ('language', 'NN'), ('too', 'RB')]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Kalimat input\n",
        "sentence = \"I am currently learning NLP in English, but if possible I want to know NLP in Indonesian language too\"\n",
        "\n",
        "# Tokenisasi dan POS tagging\n",
        "tokens = word_tokenize(sentence)\n",
        "tagged = pos_tag(tokens)\n",
        "\n",
        "# Menampilkan hasil POS tagging\n",
        "print(tagged)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0bcli9SGdhy"
      },
      "source": [
        "## Stopword removal\n",
        "\n",
        "<p> Stopword removal merupakan salah satu cara untuk melakukan normalisasi pada level kata </p>\n",
        "<p><u>Di Text Mining</u> kata-kata yang <strong>sering muncul </strong>(dan jarang sekali muncul) memiliki sedikit sekali informasi (signifikansi) terhadap model (machine learning) yang digunakan. Hal ini di karenakan kata-kata tersebut muncul di semua kategori (di permasalahan klasifikasi) atau di semua cluster (di permasalahan pengelompokan/clustering). Kata-kata yang sering muncul ini biasa disebut &quot;StopWords&quot;. Stopwords berbeda-beda bergantung dari Bahasa dan Environment (aplikasi)-nya.<br />\n",
        "<strong>Contoh</strong>:<br />\n",
        "\n",
        "<ul>\n",
        "\t<li>Stopwords bahasa Inggris: am, is, are, do, the, of, etc.</li>\n",
        "\t<li>Stopwords bahasa Indonesia: adalah, dengan, yang, di, ke, dsb</li>\n",
        "\t<li>Stopwords twitter: RT, ...</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LhNtKI5Gdhy",
        "outputId": "0a8c8dd5-1ae0-404b-9bca-b77e9b9b9168"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an']\n",
            "['ada', 'adalah', 'adanya', 'adapun', 'agak', 'agaknya', 'agar', 'akan', 'akankah', 'akhir']\n"
          ]
        }
      ],
      "source": [
        "# Contoh stopword dari NLTK\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk_stw_en = stopwords.words('english')\n",
        "print(nltk_stw_en[:10])\n",
        "\n",
        "nltk_stw_id = stopwords.words('indonesian')\n",
        "print(nltk_stw_id[:10])\n",
        "\n",
        "# Lsit stopword dapat ditambahkan dan dikurangi sesuai dengan kebutuhan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3ksoYot_a_7"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hL5Ly-zG_bbE",
        "outputId": "9bc3ece9-f6aa-4624-9fb3-8a3c584f112d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting Sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl.metadata (909 bytes)\n",
            "Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Sastrawi\n",
            "Successfully installed Sastrawi-1.0.1\n"
          ]
        }
      ],
      "source": [
        "# !pip install Sastrawi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAvt50LIGdhz",
        "outputId": "355b73d6-d450-438c-8cd2-63af19992d3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['yang', 'untuk', 'pada', 'ke', 'para', 'namun', 'menurut', 'antara', 'dia', 'dua']\n"
          ]
        }
      ],
      "source": [
        "#contoh stopword dari Sastrawi pada bahasa Indonesia\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "\n",
        "factory = StopWordRemoverFactory()\n",
        "sastrawi_stw_id = factory.get_stop_words()\n",
        "print(sastrawi_stw_id[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "65AwqowSGdhz"
      },
      "outputs": [],
      "source": [
        "# Tips:\n",
        "# selalu rubah list stopwords ke bentuk set, karena di Python jauh lebih cepat untuk cek existence di set ketimbang list\n",
        "nltk_stw_en = set(nltk_stw_en)\n",
        "nltk_stw_id = set(nltk_stw_id)\n",
        "sastrawi_stw_id = set(sastrawi_stw_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uD2PwuQBGdh0"
      },
      "source": [
        "<h3 id=\"Latihan:\"><font color=\"blue\">Latihan 7:</font></h3>\n",
        "\n",
        "<p> Lakukan stopword removal pada contoh paragraf berikut ini: </p>\n",
        "<p> \"Siti Nurbaya adalah sebuah novel Indonesia yang ditulis oleh Marah Rusli. Novel ini diterbitkan oleh Balai Pustaka, penerbit nasional negeri Hindia Belanda, pada tahun 1922.\" </p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVa7Im8nGdh0",
        "outputId": "b49564fc-d5d3-435a-a20f-61831bf16837"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Siti', 'Nurbaya', 'novel', 'Indonesia', 'ditulis', 'Marah', 'Rusli.', 'Novel', 'diterbitkan', 'Balai', 'Pustaka,', 'penerbit', 'nasional', 'negeri', 'Hindia', 'Belanda,', 'tahun', '1922.']\n"
          ]
        }
      ],
      "source": [
        "# Kerjakan latihan 7 pada cell berikut ini\n",
        "# Daftar stopwords bahasa Indonesia (dapat diperluas sesuai kebutuhan)\n",
        "stopwords_indonesia = set([\n",
        "    'yang', 'di', 'dan', 'dari', 'ke', 'untuk', 'adalah', 'pada', 'ini', 'sebuah', 'dengan', 'oleh', 'dalam', 'memiliki', 'atau'\n",
        "])\n",
        "\n",
        "# Paragraf input\n",
        "text = \"Siti Nurbaya adalah sebuah novel Indonesia yang ditulis oleh Marah Rusli. Novel ini diterbitkan oleh Balai Pustaka, penerbit nasional negeri Hindia Belanda, pada tahun 1922.\"\n",
        "\n",
        "# Tokenisasi\n",
        "tokens = text.split()\n",
        "\n",
        "# Menghapus stopwords\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stopwords_indonesia]\n",
        "\n",
        "# Menampilkan hasil\n",
        "print(filtered_tokens)\n"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
